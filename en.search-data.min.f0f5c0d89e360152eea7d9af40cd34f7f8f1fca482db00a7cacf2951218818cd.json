[{"id":0,"href":"/posts/creating-a-new-theme/","title":"Creating a New Theme","section":"Blog","content":" Introduction # This tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I\u0026rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won\u0026rsquo;t cover using CSS to style your theme.\nWe\u0026rsquo;ll start with creating a new site with a very basic template. Then we\u0026rsquo;ll add in a few pages and posts. With small variations on that, you will be able to create many different types of web sites.\nIn this tutorial, commands that you enter will start with the \u0026ldquo;$\u0026rdquo; prompt. The output will follow. Lines that start with \u0026ldquo;#\u0026rdquo; are comments that I\u0026rsquo;ve added to explain a point. When I show updates to a file, the \u0026ldquo;:wq\u0026rdquo; on the last line means to save the file.\nHere\u0026rsquo;s an example:\n## this is a comment $ echo this is a command this is a command ## edit the file $ vi foo.md +++ date = \u0026#34;2014-09-28\u0026#34; title = \u0026#34;creating a new theme\u0026#34; +++ bah and humbug :wq ## show it $ cat foo.md +++ date = \u0026#34;2014-09-28\u0026#34; title = \u0026#34;creating a new theme\u0026#34; +++ bah and humbug $ Some Definitions # There are a few concepts that you need to understand before creating a theme.\nSkins # Skins are the files responsible for the look and feel of your site. It’s the CSS that controls colors and fonts, it’s the Javascript that determines actions and reactions. It’s also the rules that Hugo uses to transform your content into the HTML that the site will serve to visitors.\nYou have two ways to create a skin. The simplest way is to create it in the layouts/ directory. If you do, then you don’t have to worry about configuring Hugo to recognize it. The first place that Hugo will look for rules and files is in the layouts/ directory so it will always find the skin.\nYour second choice is to create it in a sub-directory of the themes/ directory. If you do, then you must always tell Hugo where to search for the skin. It’s extra work, though, so why bother with it?\nThe difference between creating a skin in layouts/ and creating it in themes/ is very subtle. A skin in layouts/ can’t be customized without updating the templates and static files that it is built from. A skin created in themes/, on the other hand, can be and that makes it easier for other people to use it.\nThe rest of this tutorial will call a skin created in the themes/ directory a theme.\nNote that you can use this tutorial to create a skin in the layouts/ directory if you wish to. The main difference will be that you won’t need to update the site’s configuration file to use a theme.\nThe Home Page # The home page, or landing page, is the first page that many visitors to a site see. It is the index.html file in the root directory of the web site. Since Hugo writes files to the public/ directory, our home page is public/index.html.\nSite Configuration File # When Hugo runs, it looks for a configuration file that contains settings that override default values for the entire site. The file can use TOML, YAML, or JSON. I prefer to use TOML for my configuration files. If you prefer to use JSON or YAML, you’ll need to translate my examples. You’ll also need to change the name of the file since Hugo uses the extension to determine how to process it.\nHugo translates Markdown files into HTML. By default, Hugo expects to find Markdown files in your content/ directory and template files in your themes/ directory. It will create HTML files in your public/ directory. You can change this by specifying alternate locations in the configuration file.\nContent # Content is stored in text files that contain two sections. The first section is the “front matter,” which is the meta-information on the content. The second section contains Markdown that will be converted to HTML.\nFront Matter # The front matter is information about the content. Like the configuration file, it can be written in TOML, YAML, or JSON. Unlike the configuration file, Hugo doesn’t use the file’s extension to know the format. It looks for markers to signal the type. TOML is surrounded by “+++”, YAML by “---”, and JSON is enclosed in curly braces. I prefer to use TOML, so you’ll need to translate my examples if you prefer YAML or JSON.\nThe information in the front matter is passed into the template before the content is rendered into HTML.\nMarkdown # Content is written in Markdown which makes it easier to create the content. Hugo runs the content through a Markdown engine to create the HTML which will be written to the output file.\nTemplate Files # Hugo uses template files to render content into HTML. Template files are a bridge between the content and presentation. Rules in the template define what content is published, where it\u0026rsquo;s published to, and how it will rendered to the HTML file. The template guides the presentation by specifying the style to use.\nThere are three types of templates: single, list, and partial. Each type takes a bit of content as input and transforms it based on the commands in the template.\nHugo uses its knowledge of the content to find the template file used to render the content. If it can’t find a template that is an exact match for the content, it will shift up a level and search from there. It will continue to do so until it finds a matching template or runs out of templates to try. If it can’t find a template, it will use the default template for the site.\nPlease note that you can use the front matter to influence Hugo’s choice of templates.\nSingle Template # A single template is used to render a single piece of content. For example, an article or post would be a single piece of content and use a single template.\nList Template # A list template renders a group of related content. That could be a summary of recent postings or all articles in a category. List templates can contain multiple groups.\nThe homepage template is a special type of list template. Hugo assumes that the home page of your site will act as the portal for the rest of the content in the site.\nPartial Template # A partial template is a template that can be included in other templates. Partial templates must be called using the “partial” template command. They are very handy for rolling up common behavior. For example, your site may have a banner that all pages use. Instead of copying the text of the banner into every single and list template, you could create a partial with the banner in it. That way if you decide to change the banner, you only have to change the partial template.\nCreate a New Site # Let\u0026rsquo;s use Hugo to create a new web site. I\u0026rsquo;m a Mac user, so I\u0026rsquo;ll create mine in my home directory, in the Sites folder. If you\u0026rsquo;re using Linux, you might have to create the folder first.\nThe \u0026ldquo;new site\u0026rdquo; command will create a skeleton of a site. It will give you the basic directory structure and a useable configuration file.\n$ hugo new site ~/Sites/zafta $ cd ~/Sites/zafta $ ls -l total 8 drwxr-xr-x 7 quoha staff 238 Sep 29 16:49 . drwxr-xr-x 3 quoha staff 102 Sep 29 16:49 .. drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static $ Take a look in the content/ directory to confirm that it is empty.\nThe other directories (archetypes/, layouts/, and static/) are used when customizing a theme. That\u0026rsquo;s a topic for a different tutorial, so please ignore them for now.\nGenerate the HTML For the New Site # Running the hugo command with no options will read all the available content and generate the HTML files. It will also copy all static files (that\u0026rsquo;s everything that\u0026rsquo;s not content). Since we have an empty site, it won\u0026rsquo;t do much, but it will do it very quickly.\n$ hugo --verbose INFO: 2014/09/29 Using config file: config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ The \u0026ldquo;--verbose\u0026rdquo; flag gives extra information that will be helpful when we build the template. Every line of the output that starts with \u0026ldquo;INFO:\u0026rdquo; or \u0026ldquo;WARN:\u0026rdquo; is present because we used that flag. The lines that start with \u0026ldquo;WARN:\u0026rdquo; are warning messages. We\u0026rsquo;ll go over them later.\nWe can verify that the command worked by looking at the directory again.\n$ ls -l total 8 drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static $ See that new public/ directory? Hugo placed all generated content there. When you\u0026rsquo;re ready to publish your web site, that\u0026rsquo;s the place to start. For now, though, let\u0026rsquo;s just confirm that we have what we\u0026rsquo;d expect from a site with no content.\n$ ls -l public total 16 -rw-r--r-- 1 quoha staff 416 Sep 29 17:02 index.xml -rw-r--r-- 1 quoha staff 262 Sep 29 17:02 sitemap.xml $ Hugo created two XML files, which is standard, but there are no HTML files.\nTest the New Site # Verify that you can run the built-in web server. It will dramatically shorten your development cycle if you do. Start it by running the \u0026ldquo;server\u0026rdquo; command. If it is successful, you will see output similar to the following:\n$ hugo server --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms Serving pages from /Users/quoha/Sites/zafta/public Web Server is available at http://localhost:1313 Press Ctrl+C to stop Connect to the listed URL (it\u0026rsquo;s on the line that starts with \u0026ldquo;Web Server\u0026rdquo;). If everything is working correctly, you should get a page that shows the following:\nindex.xml sitemap.xml That\u0026rsquo;s a listing of your public/ directory. Hugo didn\u0026rsquo;t create a home page because our site has no content. When there\u0026rsquo;s no index.html file in a directory, the server lists the files in the directory, which is what you should see in your browser.\nLet’s go back and look at those warnings again.\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] That second warning is easier to explain. We haven’t created a template to be used to generate “page not found errors.” The 404 message is a topic for a separate tutorial.\nNow for the first warning. It is for the home page. You can tell because the first layout that it looked for was “index.html.” That’s only used by the home page.\nI like that the verbose flag causes Hugo to list the files that it\u0026rsquo;s searching for. For the home page, they are index.html, _default/list.html, and _default/single.html. There are some rules that we\u0026rsquo;ll cover later that explain the names and paths. For now, just remember that Hugo couldn\u0026rsquo;t find a template for the home page and it told you so.\nAt this point, you\u0026rsquo;ve got a working installation and site that we can build upon. All that’s left is to add some content and a theme to display it.\nCreate a New Theme # Hugo doesn\u0026rsquo;t ship with a default theme. There are a few available (I counted a dozen when I first installed Hugo) and Hugo comes with a command to create new themes.\nWe\u0026rsquo;re going to create a new theme called \u0026ldquo;zafta.\u0026rdquo; Since the goal of this tutorial is to show you how to fill out the files to pull in your content, the theme will not contain any CSS. In other words, ugly but functional.\nAll themes have opinions on content and layout. For example, Zafta uses \u0026ldquo;post\u0026rdquo; over \u0026ldquo;blog\u0026rdquo;. Strong opinions make for simpler templates but differing opinions make it tougher to use themes. When you build a theme, consider using the terms that other themes do.\nCreate a Skeleton # Use the hugo \u0026ldquo;new\u0026rdquo; command to create the skeleton of a theme. This creates the directory structure and places empty files for you to fill out.\n$ hugo new theme zafta $ ls -l total 8 drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static drwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes $ find themes -type f | xargs ls -l -rw-r--r-- 1 quoha staff 1081 Sep 29 17:31 themes/zafta/LICENSE.md -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/archetypes/default.md -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html -rw-r--r-- 1 quoha staff 93 Sep 29 17:31 themes/zafta/theme.toml $ The skeleton includes templates (the files ending in .html), license file, a description of your theme (the theme.toml file), and an empty archetype.\nPlease take a minute to fill out the theme.toml and LICENSE.md files. They\u0026rsquo;re optional, but if you\u0026rsquo;re going to be distributing your theme, it tells the world who to praise (or blame). It\u0026rsquo;s also nice to declare the license so that people will know how they can use the theme.\n$ vi themes/zafta/theme.toml author = \u0026#34;michael d henderson\u0026#34; description = \u0026#34;a minimal working template\u0026#34; license = \u0026#34;MIT\u0026#34; name = \u0026#34;zafta\u0026#34; source_repo = \u0026#34;\u0026#34; tags = [\u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34;] :wq ## also edit themes/zafta/LICENSE.md and change ## the bit that says \u0026#34;YOUR_NAME_HERE\u0026#34; Note that the the skeleton\u0026rsquo;s template files are empty. Don\u0026rsquo;t worry, we\u0026rsquo;ll be changing that shortly.\n$ find themes/zafta -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html $ Update the Configuration File to Use the Theme # Now that we\u0026rsquo;ve got a theme to work with, it\u0026rsquo;s a good idea to add the theme name to the configuration file. This is optional, because you can always add \u0026ldquo;-t zafta\u0026rdquo; on all your commands. I like to put it the configuration file because I like shorter command lines. If you don\u0026rsquo;t put it in the configuration file or specify it on the command line, you won\u0026rsquo;t use the template that you\u0026rsquo;re expecting to.\nEdit the file to add the theme, add a title for the site, and specify that all of our content will use the TOML format.\n$ vi config.toml theme = \u0026#34;zafta\u0026#34; baseurl = \u0026#34;\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;zafta - totally refreshing\u0026#34; MetaDataFormat = \u0026#34;toml\u0026#34; :wq $ Generate the Site # Now that we have an empty theme, let\u0026rsquo;s generate the site again.\n$ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ Did you notice that the output is different? The warning message for the home page has disappeared and we have an additional information line saying that Hugo is syncing from the theme\u0026rsquo;s directory.\nLet\u0026rsquo;s check the public/ directory to see what Hugo\u0026rsquo;s created.\n$ ls -l public total 16 drwxr-xr-x 2 quoha staff 68 Sep 29 17:56 css -rw-r--r-- 1 quoha staff 0 Sep 29 17:56 index.html -rw-r--r-- 1 quoha staff 407 Sep 29 17:56 index.xml drwxr-xr-x 2 quoha staff 68 Sep 29 17:56 js -rw-r--r-- 1 quoha staff 243 Sep 29 17:56 sitemap.xml $ Notice four things:\nHugo created a home page. This is the file public/index.html. Hugo created a css/ directory. Hugo created a js/ directory. Hugo claimed that it created 0 pages. It created a file and copied over static files, but didn\u0026rsquo;t create any pages. That\u0026rsquo;s because it considers a \u0026ldquo;page\u0026rdquo; to be a file created directly from a content file. It doesn\u0026rsquo;t count things like the index.html files that it creates automatically. The Home Page # Hugo supports many different types of templates. The home page is special because it gets its own type of template and its own template file. The file, layouts/index.html, is used to generate the HTML for the home page. The Hugo documentation says that this is the only required template, but that depends. Hugo\u0026rsquo;s warning message shows that it looks for three different templates:\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] If it can\u0026rsquo;t find any of these, it completely skips creating the home page. We noticed that when we built the site without having a theme installed.\nWhen Hugo created our theme, it created an empty home page template. Now, when we build the site, Hugo finds the template and uses it to generate the HTML for the home page. Since the template file is empty, the HTML file is empty, too. If the template had any rules in it, then Hugo would have used them to generate the home page.\n$ find . -name index.html | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 20:21 ./public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 ./themes/zafta/layouts/index.html $ The Magic of Static # Hugo does two things when generating the site. It uses templates to transform content into HTML and it copies static files into the site. Unlike content, static files are not transformed. They are copied exactly as they are.\nHugo assumes that your site will use both CSS and JavaScript, so it creates directories in your theme to hold them. Remember opinions? Well, Hugo\u0026rsquo;s opinion is that you\u0026rsquo;ll store your CSS in a directory named css/ and your JavaScript in a directory named js/. If you don\u0026rsquo;t like that, you can change the directory names in your theme directory or even delete them completely. Hugo\u0026rsquo;s nice enough to offer its opinion, then behave nicely if you disagree.\n$ find themes/zafta -type d | xargs ls -ld drwxr-xr-x 7 quoha staff 238 Sep 29 17:38 themes/zafta drwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes/zafta/archetypes drwxr-xr-x 5 quoha staff 170 Sep 29 17:31 themes/zafta/layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/_default drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/partials drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/static drwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/css drwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/js $ The Theme Development Cycle # When you\u0026rsquo;re working on a theme, you will make changes in the theme\u0026rsquo;s directory, rebuild the site, and check your changes in the browser. Hugo makes this very easy:\nPurge the public/ directory. Run the built in web server in watch mode. Open your site in a browser. Update the theme. Glance at your browser window to see changes. Return to step 4. I’ll throw in one more opinion: never work on a theme on a live site. Always work on a copy of your site. Make changes to your theme, test them, then copy them up to your site. For added safety, use a tool like Git to keep a revision history of your content and your theme. Believe me when I say that it is too easy to lose both your mind and your changes.\nCheck the main Hugo site for information on using Git with Hugo.\nPurge the public/ Directory # When generating the site, Hugo will create new files and update existing ones in the public/ directory. It will not delete files that are no longer used. For example, files that were created in the wrong directory or with the wrong title will remain. If you leave them, you might get confused by them later. I recommend cleaning out your site prior to generating it.\nNote: If you\u0026rsquo;re building on an SSD, you should ignore this. Churning on a SSD can be costly.\nHugo\u0026rsquo;s Watch Option # Hugo\u0026rsquo;s \u0026ldquo;--watch\u0026rdquo; option will monitor the content/ and your theme directories for changes and rebuild the site automatically.\nLive Reload # Hugo\u0026rsquo;s built in web server supports live reload. As pages are saved on the server, the browser is told to refresh the page. Usually, this happens faster than you can say, \u0026ldquo;Wow, that\u0026rsquo;s totally amazing.\u0026rdquo;\nDevelopment Commands # Use the following commands as the basis for your workflow.\n## purge old files. hugo will recreate the public directory. ## $ rm -rf public ## ## run hugo in watch mode ## $ hugo server --watch --verbose Here\u0026rsquo;s sample output showing Hugo detecting a change to the template for the home page. Once generated, the web browser automatically reloaded the page. I\u0026rsquo;ve said this before, it\u0026rsquo;s amazing.\n$ rm -rf public $ hugo server --watch --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms Watching for changes in /Users/quoha/Sites/zafta/content Serving pages from /Users/quoha/Sites/zafta/public Web Server is available at http://localhost:1313 Press Ctrl+C to stop INFO: 2014/09/29 File System Event: [\u0026#34;/Users/quoha/Sites/zafta/themes/zafta/layouts/index.html\u0026#34;: MODIFY|ATTRIB] Change detected, rebuilding site WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 1 ms Update the Home Page Template # The home page is one of a few special pages that Hugo creates automatically. As mentioned earlier, it looks for one of three files in the theme\u0026rsquo;s layout/ directory:\nindex.html _default/list.html _default/single.html We could update one of the default templates, but a good design decision is to update the most specific template available. That\u0026rsquo;s not a hard and fast rule (in fact, we\u0026rsquo;ll break it a few times in this tutorial), but it is a good generalization.\nMake a Static Home Page # Right now, that page is empty because we don\u0026rsquo;t have any content and we don\u0026rsquo;t have any logic in the template. Let\u0026rsquo;s change that by adding some text to the template.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq $ Build the web site and then verify the results.\n$ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 78 Sep 29 21:26 public/index.html $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;/html\u0026gt; Live Reload # Note: If you\u0026rsquo;re running the server with the --watch option, you\u0026rsquo;ll see different content in the file:\n$ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;script\u0026gt;document.write(\u0026#39;\u0026lt;script src=\u0026#34;http://\u0026#39; + (location.host || \u0026#39;localhost\u0026#39;).split(\u0026#39;:\u0026#39;)[0] + \u0026#39;:1313/livereload.js?mindelay=10\u0026#34;\u0026gt;\u0026lt;/\u0026#39; + \u0026#39;script\u0026gt;\u0026#39;)\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; When you use --watch, the Live Reload script is added by Hugo. Look for live reload in the documentation to see what it does and how to disable it.\nBuild a \u0026ldquo;Dynamic\u0026rdquo; Home Page # \u0026ldquo;Dynamic home page?\u0026rdquo; Hugo\u0026rsquo;s a static web site generator, so this seems an odd thing to say. I mean let\u0026rsquo;s have the home page automatically reflect the content in the site every time Hugo builds it. We\u0026rsquo;ll use iteration in the template to do that.\nCreate New Posts # Now that we have the home page generating static content, let\u0026rsquo;s add some content to the site. We\u0026rsquo;ll display these posts as a list on the home page and on their own page, too.\nHugo has a command to generate a skeleton post, just like it does for sites and themes.\n$ hugo --verbose new post/first.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/first.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/default.md ERROR: 2014/09/29 Unable to Cast \u0026lt;nil\u0026gt; to map[string]interface{} $ That wasn\u0026rsquo;t very nice, was it?\nThe \u0026ldquo;new\u0026rdquo; command uses an archetype to create the post file. Hugo created an empty default archetype file, but that causes an error when there\u0026rsquo;s a theme. For me, the workaround was to create an archetypes file specifically for the post type.\n$ vi themes/zafta/archetypes/post.md +++ Description = \u0026#34;\u0026#34; Tags = [] Categories = [] +++ :wq $ find themes/zafta/archetypes -type f | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 21:53 themes/zafta/archetypes/default.md -rw-r--r-- 1 quoha staff 51 Sep 29 21:54 themes/zafta/archetypes/post.md $ hugo --verbose new post/first.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/first.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md INFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/first.md /Users/quoha/Sites/zafta/content/post/first.md created $ hugo --verbose new post/second.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/second.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md INFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/second.md /Users/quoha/Sites/zafta/content/post/second.md created $ ls -l content/post total 16 -rw-r--r-- 1 quoha staff 104 Sep 29 21:54 first.md -rw-r--r-- 1 quoha staff 105 Sep 29 21:57 second.md $ cat content/post/first.md +++ Categories = [] Description = \u0026#34;\u0026#34; Tags = [] date = \u0026#34;2014-09-29T21:54:53-05:00\u0026#34; title = \u0026#34;first\u0026#34; +++ my first post $ cat content/post/second.md +++ Categories = [] Description = \u0026#34;\u0026#34; Tags = [] date = \u0026#34;2014-09-29T21:57:09-05:00\u0026#34; title = \u0026#34;second\u0026#34; +++ my second post $ Build the web site and then verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;, \u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ The output says that it created 2 pages. Those are our new posts:\n$ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 78 Sep 29 22:13 public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/second/index.html $ The new files are empty because because the templates used to generate the content are empty. The homepage doesn\u0026rsquo;t show the new content, either. We have to update the templates to add the posts.\nList and Single Templates # In Hugo, we have three major kinds of templates. There\u0026rsquo;s the home page template that we updated previously. It is used only by the home page. We also have \u0026ldquo;single\u0026rdquo; templates which are used to generate output for a single content file. We also have \u0026ldquo;list\u0026rdquo; templates that are used to group multiple pieces of content before generating output.\nGenerally speaking, list templates are named \u0026ldquo;list.html\u0026rdquo; and single templates are named \u0026ldquo;single.html.\u0026rdquo;\nThere are three other types of templates: partials, content views, and terms. We will not go into much detail on these.\nAdd Content to the Homepage # The home page will contain a list of posts. Let\u0026rsquo;s update its template to add the posts that we just created. The logic in the template will run every time we build the site.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; {{ range first 10 .Data.Pages }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq $ Hugo uses the Go template engine. That engine scans the template files for commands which are enclosed between \u0026ldquo;{{\u0026rdquo; and \u0026ldquo;}}\u0026rdquo;. In our template, the commands are:\nrange .Title end The \u0026ldquo;range\u0026rdquo; command is an iterator. We\u0026rsquo;re going to use it to go through the first ten pages. Every HTML file that Hugo creates is treated as a page, so looping through the list of pages will look at every file that will be created.\nThe \u0026ldquo;.Title\u0026rdquo; command prints the value of the \u0026ldquo;title\u0026rdquo; variable. Hugo pulls it from the front matter in the Markdown file.\nThe \u0026ldquo;end\u0026rdquo; command signals the end of the range iterator. The engine loops back to the top of the iteration when it finds \u0026ldquo;end.\u0026rdquo; Everything between the \u0026ldquo;range\u0026rdquo; and \u0026ldquo;end\u0026rdquo; is evaluated every time the engine goes through the iteration. In this file, that would cause the title from the first ten pages to be output as heading level one.\nIt\u0026rsquo;s helpful to remember that some variables, like .Data, are created before any output files. Hugo loads every content file into the variable and then gives the template a chance to process before creating the HTML files.\nBuild the web site and then verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:23 public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/second/index.html $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;second\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;first\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ Congratulations, the home page shows the title of the two posts. The posts themselves are still empty, but let\u0026rsquo;s take a moment to appreciate what we\u0026rsquo;ve done. Your template now generates output dynamically. Believe it or not, by inserting the range command inside of those curly braces, you\u0026rsquo;ve learned everything you need to know to build a theme. All that\u0026rsquo;s really left is understanding which template will be used to generate each content file and becoming familiar with the commands for the template engine.\nAnd, if that were entirely true, this tutorial would be much shorter. There are a few things to know that will make creating a new template much easier. Don\u0026rsquo;t worry, though, that\u0026rsquo;s all to come.\nAdd Content to the Posts # We\u0026rsquo;re working with posts, which are in the content/post/ directory. That means that their section is \u0026ldquo;post\u0026rdquo; (and if we don\u0026rsquo;t do something weird, their type is also \u0026ldquo;post\u0026rdquo;).\nHugo uses the section and type to find the template file for every piece of content. Hugo will first look for a template file that matches the section or type name. If it can\u0026rsquo;t find one, then it will look in the _default/ directory. There are some twists that we\u0026rsquo;ll cover when we get to categories and tags, but for now we can assume that Hugo will try post/single.html, then _default/single.html.\nNow that we know the search rule, let\u0026rsquo;s see what we actually have available:\n$ find themes/zafta -name single.html | xargs ls -l -rw-r--r-- 1 quoha staff 132 Sep 29 17:31 themes/zafta/layouts/_default/single.html We could create a new template, post/single.html, or change the default. Since we don\u0026rsquo;t know of any other content types, let\u0026rsquo;s start with updating the default.\nRemember, any content that we haven\u0026rsquo;t created a template for will end up using this template. That can be good or bad. Bad because I know that we\u0026rsquo;re going to be adding different types of content and we\u0026rsquo;re going to end up undoing some of the changes we\u0026rsquo;ve made. It\u0026rsquo;s good because we\u0026rsquo;ll be able to see immediate results. It\u0026rsquo;s also good to start here because we can start to build the basic layout for the site. As we add more content types, we\u0026rsquo;ll refactor this file and move logic around. Hugo makes that fairly painless, so we\u0026rsquo;ll accept the cost and proceed.\nPlease see the Hugo documentation on template rendering for all the details on determining which template to use. And, as the docs mention, if you\u0026rsquo;re building a single page application (SPA) web site, you can delete all of the other templates and work with just the default single page. That\u0026rsquo;s a refreshing amount of joy right there.\nUpdate the Template File # $ vi themes/zafta/layouts/_default/single.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq $ Build the web site and verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:40 public/index.html -rw-r--r-- 1 quoha staff 125 Sep 29 22:40 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:40 public/post/index.html -rw-r--r-- 1 quoha staff 128 Sep 29 22:40 public/post/second/index.html $ cat public/post/first/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;first\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;first\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;my first post\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ cat public/post/second/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;second\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;second\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;my second post\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ Notice that the posts now have content. You can go to localhost:1313/post/first to verify.\nLinking to Content # The posts are on the home page. Let\u0026rsquo;s add a link from there to the post. Since this is the home page, we\u0026rsquo;ll update its template.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; {{ range first 10 .Data.Pages }} \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Build the web site and verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 149 Sep 29 22:44 public/index.html -rw-r--r-- 1 quoha staff 125 Sep 29 22:44 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:44 public/post/index.html -rw-r--r-- 1 quoha staff 128 Sep 29 22:44 public/post/second/index.html $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;/post/second/\u0026#34;\u0026gt;second\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;/post/first/\u0026#34;\u0026gt;first\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ Create a Post Listing # We have the posts displaying on the home page and on their own page. We also have a file public/post/index.html that is empty. Let\u0026rsquo;s make it show a list of all posts (not just the first ten).\nWe need to decide which template to update. This will be a listing, so it should be a list template. Let\u0026rsquo;s take a quick look and see which list templates are available.\n$ find themes/zafta -name list.html | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html As with the single post, we have to decide to update _default/list.html or create post/list.html. We still don\u0026rsquo;t have multiple content types, so let\u0026rsquo;s stay consistent and update the default list template.\nCreating Top Level Pages # Let\u0026rsquo;s add an \u0026ldquo;about\u0026rdquo; page and display it at the top level (as opposed to a sub-level like we did with posts).\nThe default in Hugo is to use the directory structure of the content/ directory to guide the location of the generated html in the public/ directory. Let\u0026rsquo;s verify that by creating an \u0026ldquo;about\u0026rdquo; page at the top level:\n$ vi content/about.md +++ title = \u0026#34;about\u0026#34; description = \u0026#34;about this site\u0026#34; date = \u0026#34;2014-09-27\u0026#34; slug = \u0026#34;about time\u0026#34; +++ ## about us i\u0026#39;m speechless :wq Generate the web site and verify the results.\n$ find public -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-rw-r-- 1 mdhender staff 334 Sep 27 15:08 public/about-time/index.html -rw-rw-r-- 1 mdhender staff 527 Sep 27 15:08 public/index.html -rw-rw-r-- 1 mdhender staff 358 Sep 27 15:08 public/post/first-post/index.html -rw-rw-r-- 1 mdhender staff 0 Sep 27 15:08 public/post/index.html -rw-rw-r-- 1 mdhender staff 342 Sep 27 15:08 public/post/second-post/index.html Notice that the page wasn\u0026rsquo;t created at the top level. It was created in a sub-directory named \u0026lsquo;about-time/\u0026rsquo;. That name came from our slug. Hugo will use the slug to name the generated content. It\u0026rsquo;s a reasonable default, by the way, but we can learn a few things by fighting it for this file.\nOne other thing. Take a look at the home page.\n$ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/post/theme/\u0026#34;\u0026gt;creating a new theme\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/about-time/\u0026#34;\u0026gt;about\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/post/second-post/\u0026#34;\u0026gt;second\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/post/first-post/\u0026#34;\u0026gt;first\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;script\u0026gt;document.write(\u0026#39;\u0026lt;script src=\u0026#34;http://\u0026#39; + (location.host || \u0026#39;localhost\u0026#39;).split(\u0026#39;:\u0026#39;)[0] + \u0026#39;:1313/livereload.js?mindelay=10\u0026#34;\u0026gt;\u0026lt;/\u0026#39; + \u0026#39;script\u0026gt;\u0026#39;)\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Notice that the \u0026ldquo;about\u0026rdquo; link is listed with the posts? That\u0026rsquo;s not desirable, so let\u0026rsquo;s change that first.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;posts\u0026lt;/h1\u0026gt; {{ range first 10 .Data.Pages }} {{ if eq .Type \u0026#34;post\u0026#34;}} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;h1\u0026gt;pages\u0026lt;/h1\u0026gt; {{ range .Data.Pages }} {{ if eq .Type \u0026#34;page\u0026#34; }} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq Generate the web site and verify the results. The home page has two sections, posts and pages, and each section has the right set of headings and links in it.\nBut, that about page still renders to about-time/index.html.\n$ find public -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-rw-r-- 1 mdhender staff 334 Sep 27 15:33 public/about-time/index.html -rw-rw-r-- 1 mdhender staff 645 Sep 27 15:33 public/index.html -rw-rw-r-- 1 mdhender staff 358 Sep 27 15:33 public/post/first-post/index.html -rw-rw-r-- 1 mdhender staff 0 Sep 27 15:33 public/post/index.html -rw-rw-r-- 1 mdhender staff 342 Sep 27 15:33 public/post/second-post/index.html Knowing that hugo is using the slug to generate the file name, the simplest solution is to change the slug. Let\u0026rsquo;s do it the hard way and change the permalink in the configuration file.\n$ vi config.toml [permalinks] page = \u0026#34;/:title/\u0026#34; about = \u0026#34;/:filename/\u0026#34; Generate the web site and verify that this didn\u0026rsquo;t work. Hugo lets \u0026ldquo;slug\u0026rdquo; or \u0026ldquo;URL\u0026rdquo; override the permalinks setting in the configuration file. Go ahead and comment out the slug in content/about.md, then generate the web site to get it to be created in the right place.\nSharing Templates # If you\u0026rsquo;ve been following along, you probably noticed that posts have titles in the browser and the home page doesn\u0026rsquo;t. That\u0026rsquo;s because we didn\u0026rsquo;t put the title in the home page\u0026rsquo;s template (layouts/index.html). That\u0026rsquo;s an easy thing to do, but let\u0026rsquo;s look at a different option.\nWe can put the common bits into a shared template that\u0026rsquo;s stored in the themes/zafta/layouts/partials/ directory.\nCreate the Header and Footer Partials # In Hugo, a partial is a sugar-coated template. Normally a template reference has a path specified. Partials are different. Hugo searches for them along a TODO defined search path. This makes it easier for end-users to override the theme\u0026rsquo;s presentation.\n$ vi themes/zafta/layouts/partials/header.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; :wq $ vi themes/zafta/layouts/partials/footer.html \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq Update the Home Page Template to Use the Partials # The most noticeable difference between a template call and a partials call is the lack of path:\n{{ template \u0026#34;theme/partials/header.html\u0026#34; . }} versus\n{{ partial \u0026#34;header.html\u0026#34; . }} Both pass in the context.\nLet\u0026rsquo;s change the home page template to use these new partials.\n$ vi themes/zafta/layouts/index.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;posts\u0026lt;/h1\u0026gt; {{ range first 10 .Data.Pages }} {{ if eq .Type \u0026#34;post\u0026#34;}} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;h1\u0026gt;pages\u0026lt;/h1\u0026gt; {{ range .Data.Pages }} {{ if or (eq .Type \u0026#34;page\u0026#34;) (eq .Type \u0026#34;about\u0026#34;) }} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Type }} - {{ .Title }} - {{ .RelPermalink }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Generate the web site and verify the results. The title on the home page is now \u0026ldquo;your title here\u0026rdquo;, which comes from the \u0026ldquo;title\u0026rdquo; variable in the config.toml file.\nUpdate the Default Single Template to Use the Partials # $ vi themes/zafta/layouts/_default/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Generate the web site and verify the results. The title on the posts and the about page should both reflect the value in the markdown file.\nAdd “Date Published” to Posts # It\u0026rsquo;s common to have posts display the date that they were written or published, so let\u0026rsquo;s add that. The front matter of our posts has a variable named \u0026ldquo;date.\u0026rdquo; It\u0026rsquo;s usually the date the content was created, but let\u0026rsquo;s pretend that\u0026rsquo;s the value we want to display.\nAdd “Date Published” to the Template # We\u0026rsquo;ll start by updating the template used to render the posts. The template code will look like:\n{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }} Posts use the default single template, so we\u0026rsquo;ll change that file.\n$ vi themes/zafta/layouts/_default/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }}\u0026lt;/h2\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Generate the web site and verify the results. The posts now have the date displayed in them. There\u0026rsquo;s a problem, though. The \u0026ldquo;about\u0026rdquo; page also has the date displayed.\nAs usual, there are a couple of ways to make the date display only on posts. We could do an \u0026ldquo;if\u0026rdquo; statement like we did on the home page. Another way would be to create a separate template for posts.\nThe \u0026ldquo;if\u0026rdquo; solution works for sites that have just a couple of content types. It aligns with the principle of \u0026ldquo;code for today,\u0026rdquo; too.\nLet\u0026rsquo;s assume, though, that we\u0026rsquo;ve made our site so complex that we feel we have to create a new template type. In Hugo-speak, we\u0026rsquo;re going to create a section template.\nLet\u0026rsquo;s restore the default single template before we forget.\n$ mkdir themes/zafta/layouts/post $ vi themes/zafta/layouts/_default/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Now we\u0026rsquo;ll update the post\u0026rsquo;s version of the single template. If you remember Hugo\u0026rsquo;s rules, the template engine will use this version over the default.\n$ vi themes/zafta/layouts/post/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }}\u0026lt;/h2\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} :wq Note that we removed the date logic from the default template and put it in the post template. Generate the web site and verify the results. Posts have dates and the about page doesn\u0026rsquo;t.\nDon\u0026rsquo;t Repeat Yourself # DRY is a good design goal and Hugo does a great job supporting it. Part of the art of a good template is knowing when to add a new template and when to update an existing one. While you\u0026rsquo;re figuring that out, accept that you\u0026rsquo;ll be doing some refactoring. Hugo makes that easy and fast, so it\u0026rsquo;s okay to delay splitting up a template.\n"},{"id":1,"href":"/posts/migrate-from-jekyll/","title":"Migrating from Jekyll","section":"Blog","content":" Move static content to static # Jekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like\n▾ \u0026lt;root\u0026gt;/ ▾ images/ logo.png should become\n▾ \u0026lt;root\u0026gt;/ ▾ static/ ▾ images/ logo.png Additionally, you\u0026rsquo;ll want any files that should reside at the root (such as CNAME) to be moved to static.\nCreate your Hugo configuration file # Hugo can read your configuration as JSON, YAML or TOML. Hugo supports parameters custom configuration too. Refer to the Hugo configuration documentation for details.\nSet your configuration publish folder to _site # The default is for Jekyll to publish to _site and for Hugo to publish to public. If, like me, you have _site mapped to a git submodule on the gh-pages branch, you\u0026rsquo;ll want to do one of two alternatives:\nChange your submodule to point to map gh-pages to public instead of _site (recommended).\ngit submodule deinit _site git rm _site git submodule add -b gh-pages git@github.com:your-username/your-repo.git public Or, change the Hugo configuration to use _site instead of public.\n{ .. \u0026quot;publishdir\u0026quot;: \u0026quot;_site\u0026quot;, .. } Convert Jekyll templates to Hugo templates # That\u0026rsquo;s the bulk of the work right here. The documentation is your friend. You should refer to Jekyll\u0026rsquo;s template documentation if you need to refresh your memory on how you built your blog and Hugo\u0026rsquo;s template to learn Hugo\u0026rsquo;s way.\nAs a single reference data point, converting my templates for heyitsalex.net took me no more than a few hours.\nConvert Jekyll plugins to Hugo shortcodes # Jekyll has plugins; Hugo has shortcodes. It\u0026rsquo;s fairly trivial to do a port.\nImplementation # As an example, I was using a custom image_tag plugin to generate figures with caption when running Jekyll. As I read about shortcodes, I found Hugo had a nice built-in shortcode that does exactly the same thing.\nJekyll\u0026rsquo;s plugin:\nmodule Jekyll class ImageTag \u0026lt; Liquid::Tag @url = nil @caption = nil @class = nil @link = nil // Patterns IMAGE_URL_WITH_CLASS_AND_CAPTION = IMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;(\\s+)-\u0026gt;((https?:\\/\\/|\\/)(\\S+))(\\s*)/i IMAGE_URL_WITH_CAPTION = /((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;/i IMAGE_URL_WITH_CLASS = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))/i IMAGE_URL = /((https?:\\/\\/|\\/)(\\S+))/i def initialize(tag_name, markup, tokens) super if markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK @class = $1 @url = $3 @caption = $7 @link = $9 elsif markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION @class = $1 @url = $3 @caption = $7 elsif markup =~ IMAGE_URL_WITH_CAPTION @url = $1 @caption = $5 elsif markup =~ IMAGE_URL_WITH_CLASS @class = $1 @url = $3 elsif markup =~ IMAGE_URL @url = $1 end end def render(context) if @class source = \u0026quot;\u0026lt;figure class='#{@class}'\u0026gt;\u0026quot; else source = \u0026quot;\u0026lt;figure\u0026gt;\u0026quot; end if @link source += \u0026quot;\u0026lt;a href=\\\u0026quot;#{@link}\\\u0026quot;\u0026gt;\u0026quot; end source += \u0026quot;\u0026lt;img src=\\\u0026quot;#{@url}\\\u0026quot;\u0026gt;\u0026quot; if @link source += \u0026quot;\u0026lt;/a\u0026gt;\u0026quot; end source += \u0026quot;\u0026lt;figcaption\u0026gt;#{@caption}\u0026lt;/figcaption\u0026gt;\u0026quot; if @caption source += \u0026quot;\u0026lt;/figure\u0026gt;\u0026quot; source end end end Liquid::Template.register_tag('image', Jekyll::ImageTag) is written as this Hugo shortcode:\n\u0026lt;!-- image --\u0026gt; \u0026lt;figure {{ with .Get \u0026quot;class\u0026quot; }}class=\u0026quot;{{.}}\u0026quot;{{ end }}\u0026gt; {{ with .Get \u0026quot;link\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt;{{ end }} \u0026lt;img src=\u0026quot;{{ .Get \u0026quot;src\u0026quot; }}\u0026quot; {{ if or (.Get \u0026quot;alt\u0026quot;) (.Get \u0026quot;caption\u0026quot;) }}alt=\u0026quot;{{ with .Get \u0026quot;alt\u0026quot;}}{{.}}{{else}}{{ .Get \u0026quot;caption\u0026quot; }}{{ end }}\u0026quot;{{ end }} /\u0026gt; {{ if .Get \u0026quot;link\u0026quot;}}\u0026lt;/a\u0026gt;{{ end }} {{ if or (or (.Get \u0026quot;title\u0026quot;) (.Get \u0026quot;caption\u0026quot;)) (.Get \u0026quot;attr\u0026quot;)}} \u0026lt;figcaption\u0026gt;{{ if isset .Params \u0026quot;title\u0026quot; }} {{ .Get \u0026quot;title\u0026quot; }}{{ end }} {{ if or (.Get \u0026quot;caption\u0026quot;) (.Get \u0026quot;attr\u0026quot;)}}\u0026lt;p\u0026gt; {{ .Get \u0026quot;caption\u0026quot; }} {{ with .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt; {{ end }} {{ .Get \u0026quot;attr\u0026quot; }} {{ if .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/p\u0026gt; {{ end }} \u0026lt;/figcaption\u0026gt; {{ end }} \u0026lt;/figure\u0026gt; \u0026lt;!-- image --\u0026gt; Usage # I simply changed:\n{% image full http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg \u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were \u0026quot;having fun\u0026quot; and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; -\u0026gt;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/ %} to this (this example uses a slightly extended version named fig, different than the built-in figure):\n{{% fig class=\u0026quot;full\u0026quot; src=\u0026quot;http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg\u0026quot; title=\u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were having fun and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; link=\u0026quot;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/\u0026quot; %}} As a bonus, the shortcode named parameters are, arguably, more readable.\nFinishing touches # Fix content # Depending on the amount of customization that was done with each post with Jekyll, this step will require more or less effort. There are no hard and fast rules here except that hugo server --watch is your friend. Test your changes and fix errors as needed.\nClean up # You\u0026rsquo;ll want to remove the Jekyll configuration at this point. If you have anything else that isn\u0026rsquo;t used, delete it.\nA practical example in a diff # Hey, it\u0026rsquo;s Alex was migrated in less than a father-with-kids day from Jekyll to Hugo. You can see all the changes (and screw-ups) by looking at this diff.\n"},{"id":2,"href":"/posts/goisforlovers/","title":"(Hu)go Template Primer","section":"Blog","content":"Hugo uses the excellent Go html/template library for its template engine. It is an extremely lightweight engine that provides a very small amount of logic. In our experience that it is just the right amount of logic to be able to create a good static website. If you have used other template systems from different languages or frameworks you will find a lot of similarities in Go templates.\nThis document is a brief primer on using Go templates. The Go docs provide more details.\nIntroduction to Go Templates # Go templates provide an extremely simple template language. It adheres to the belief that only the most basic of logic belongs in the template or view layer. One consequence of this simplicity is that Go templates parse very quickly.\nA unique characteristic of Go templates is they are content aware. Variables and content will be sanitized depending on the context of where they are used. More details can be found in the Go docs.\nBasic Syntax # Golang templates are HTML files with the addition of variables and functions.\nGo variables and functions are accessible within {{ }}\nAccessing a predefined variable \u0026ldquo;foo\u0026rdquo;:\n{{ foo }} Parameters are separated using spaces\nCalling the add function with input of 1, 2:\n{{ add 1 2 }} Methods and fields are accessed via dot notation\nAccessing the Page Parameter \u0026ldquo;bar\u0026rdquo;\n{{ .Params.bar }} Parentheses can be used to group items together\n{{ if or (isset .Params \u0026quot;alt\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;) }} Caption {{ end }} Variables # Each Go template has a struct (object) made available to it. In hugo each template is passed either a page or a node struct depending on which type of page you are rendering. More details are available on the variables page.\nA variable is accessed by referencing the variable name.\n\u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; Variables can also be defined and referenced.\n{{ $address := \u0026quot;123 Main St.\u0026quot;}} {{ $address }} Functions # Go template ship with a few functions which provide basic functionality. The Go template system also provides a mechanism for applications to extend the available functions with their own. Hugo template functions provide some additional functionality we believe are useful for building websites. Functions are called by using their name followed by the required parameters separated by spaces. Template functions cannot be added without recompiling hugo.\nExample:\n{{ add 1 2 }} Includes # When including another template you will pass to it the data it will be able to access. To pass along the current context please remember to include a trailing dot. The templates location will always be starting at the /layout/ directory within Hugo.\nExample:\n{{ template \u0026quot;chrome/header.html\u0026quot; . }} Logic # Go templates provide the most basic iteration and conditional logic.\nIteration # Just like in Go, the Go templates make heavy use of range to iterate over a map, array or slice. The following are different examples of how to use range.\nExample 1: Using Context\n{{ range array }} {{ . }} {{ end }} Example 2: Declaring value variable name\n{{range $element := array}} {{ $element }} {{ end }} Example 2: Declaring key and value variable name\n{{range $index, $element := array}} {{ $index }} {{ $element }} {{ end }} Conditionals # If, else, with, or, \u0026amp; and provide the framework for handling conditional logic in Go Templates. Like range, each statement is closed with end.\nGo Templates treat the following values as false:\nfalse 0 any array, slice, map, or string of length zero Example 1: If\n{{ if isset .Params \u0026quot;title\u0026quot; }}\u0026lt;h4\u0026gt;{{ index .Params \u0026quot;title\u0026quot; }}\u0026lt;/h4\u0026gt;{{ end }} Example 2: If -\u0026gt; Else\n{{ if isset .Params \u0026quot;alt\u0026quot; }} {{ index .Params \u0026quot;alt\u0026quot; }} {{else}} {{ index .Params \u0026quot;caption\u0026quot; }} {{ end }} Example 3: And \u0026amp; Or\n{{ if and (or (isset .Params \u0026quot;title\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;)) (isset .Params \u0026quot;attr\u0026quot;)}} Example 4: With\nAn alternative way of writing \u0026ldquo;if\u0026rdquo; and then referencing the same value is to use \u0026ldquo;with\u0026rdquo; instead. With rebinds the context . within its scope, and skips the block if the variable is absent.\nThe first example above could be simplified as:\n{{ with .Params.title }}\u0026lt;h4\u0026gt;{{ . }}\u0026lt;/h4\u0026gt;{{ end }} Example 5: If -\u0026gt; Else If\n{{ if isset .Params \u0026quot;alt\u0026quot; }} {{ index .Params \u0026quot;alt\u0026quot; }} {{ else if isset .Params \u0026quot;caption\u0026quot; }} {{ index .Params \u0026quot;caption\u0026quot; }} {{ end }} Pipes # One of the most powerful components of Go templates is the ability to stack actions one after another. This is done by using pipes. Borrowed from unix pipes, the concept is simple, each pipeline\u0026rsquo;s output becomes the input of the following pipe.\nBecause of the very simple syntax of Go templates, the pipe is essential to being able to chain together function calls. One limitation of the pipes is that they only can work with a single value and that value becomes the last parameter of the next pipeline.\nA few simple examples should help convey how to use the pipe.\nExample 1 :\n{{ if eq 1 1 }} Same {{ end }} is the same as\n{{ eq 1 1 | if }} Same {{ end }} It does look odd to place the if at the end, but it does provide a good illustration of how to use the pipes.\nExample 2 :\n{{ index .Params \u0026quot;disqus_url\u0026quot; | html }} Access the page parameter called \u0026ldquo;disqus_url\u0026rdquo; and escape the HTML.\nExample 3 :\n{{ if or (or (isset .Params \u0026quot;title\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;)) (isset .Params \u0026quot;attr\u0026quot;)}} Stuff Here {{ end }} Could be rewritten as\n{{ isset .Params \u0026quot;caption\u0026quot; | or isset .Params \u0026quot;title\u0026quot; | or isset .Params \u0026quot;attr\u0026quot; | if }} Stuff Here {{ end }} Context (aka. the dot) # The most easily overlooked concept to understand about Go templates is that {{ . }} always refers to the current context. In the top level of your template this will be the data set made available to it. Inside of a iteration it will have the value of the current item. When inside of a loop the context has changed. . will no longer refer to the data available to the entire page. If you need to access this from within the loop you will likely want to set it to a variable instead of depending on the context.\nExample:\n{{ $title := .Site.Title }} {{ range .Params.tags }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;{{ $baseurl }}/tags/{{ . | urlize }}\u0026quot;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; - {{ $title }} \u0026lt;/li\u0026gt; {{ end }} Notice how once we have entered the loop the value of {{ . }} has changed. We have defined a variable outside of the loop so we have access to it from within the loop.\nHugo Parameters # Hugo provides the option of passing values to the template language through the site configuration (for sitewide values), or through the meta data of each specific piece of content. You can define any values of any type (supported by your front matter/config format) and use them however you want to inside of your templates.\nUsing Content (page) Parameters # In each piece of content you can provide variables to be used by the templates. This happens in the front matter.\nAn example of this is used in this documentation site. Most of the pages benefit from having the table of contents provided. Sometimes the TOC just doesn\u0026rsquo;t make a lot of sense. We\u0026rsquo;ve defined a variable in our front matter of some pages to turn off the TOC from being displayed.\nHere is the example front matter:\n--- title: \u0026#34;Permalinks\u0026#34; date: \u0026#34;2013-11-18\u0026#34; aliases: - \u0026#34;/doc/permalinks/\u0026#34; groups: [\u0026#34;extras\u0026#34;] groups_weight: 30 notoc: true --- Here is the corresponding code inside of the template:\n{{ if not .Params.notoc }} \u0026lt;div id=\u0026quot;toc\u0026quot; class=\u0026quot;well col-md-4 col-sm-6\u0026quot;\u0026gt; {{ .TableOfContents }} \u0026lt;/div\u0026gt; {{ end }} Using Site (config) Parameters # In your top-level configuration file (eg, config.yaml) you can define site parameters, which are values which will be available to you in chrome.\nFor instance, you might declare:\nparams: CopyrightHTML: \u0026#34;Copyright \u0026amp;#xA9; 2013 John Doe. All Rights Reserved.\u0026#34; TwitterUser: \u0026#34;spf13\u0026#34; SidebarRecentLimit: 5 Within a footer layout, you might then declare a \u0026lt;footer\u0026gt; which is only provided if the CopyrightHTML parameter is provided, and if it is given, you would declare it to be HTML-safe, so that the HTML entity is not escaped again. This would let you easily update just your top-level config file each January 1st, instead of hunting through your templates.\n{{if .Site.Params.CopyrightHTML}}\u0026lt;footer\u0026gt; \u0026lt;div class=\u0026#34;text-center\u0026#34;\u0026gt;{{.Site.Params.CopyrightHTML | safeHtml}}\u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt;{{end}} An alternative way of writing the \u0026ldquo;if\u0026rdquo; and then referencing the same value is to use \u0026ldquo;with\u0026rdquo; instead. With rebinds the context . within its scope, and skips the block if the variable is absent:\n{{with .Site.Params.TwitterUser}}\u0026lt;span class=\u0026#34;twitter\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;https://twitter.com/{{.}}\u0026#34; rel=\u0026#34;author\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;/images/twitter.png\u0026#34; width=\u0026#34;48\u0026#34; height=\u0026#34;48\u0026#34; title=\u0026#34;Twitter: {{.}}\u0026#34; alt=\u0026#34;Twitter\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/span\u0026gt;{{end}} Finally, if you want to pull \u0026ldquo;magic constants\u0026rdquo; out of your layouts, you can do so, such as in this example:\n\u0026lt;nav class=\u0026#34;recent\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Recent Posts\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt;{{range first .Site.Params.SidebarRecentLimit .Site.Recent}} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{.RelPermalink}}\u0026#34;\u0026gt;{{.Title}}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {{end}}\u0026lt;/ul\u0026gt; \u0026lt;/nav\u0026gt; "},{"id":3,"href":"/posts/hugoisforlovers/","title":"Getting Started with Hugo","section":"Blog","content":" Step 1. Install Hugo # Go to Hugo releases and download the appropriate version for your OS and architecture.\nSave it somewhere specific as we will be using it in the next step.\nMore complete instructions are available at Install Hugo\nStep 2. Build the Docs # Hugo has its own example site which happens to also be the documentation site you are reading right now.\nFollow the following steps:\nClone the Hugo repository Go into the repo Run hugo in server mode and build the docs Open your browser to http://localhost:1313 Corresponding pseudo commands:\ngit clone https://github.com/spf13/hugo cd hugo /path/to/where/you/installed/hugo server --source=./docs \u0026gt; 29 pages created \u0026gt; 0 tags index created \u0026gt; in 27 ms \u0026gt; Web Server is available at http://localhost:1313 \u0026gt; Press ctrl+c to stop Once you\u0026rsquo;ve gotten here, follow along the rest of this page on your local build.\nStep 3. Change the docs site # Stop the Hugo process by hitting Ctrl+C.\nNow we are going to run hugo again, but this time with hugo in watch mode.\n/path/to/hugo/from/step/1/hugo server --source=./docs --watch \u0026gt; 29 pages created \u0026gt; 0 tags index created \u0026gt; in 27 ms \u0026gt; Web Server is available at http://localhost:1313 \u0026gt; Watching for changes in /Users/spf13/Code/hugo/docs/content \u0026gt; Press ctrl+c to stop Open your favorite editor and change one of the source content pages. How about changing this very file to fix the typo. How about changing this very file to fix the typo.\nContent files are found in docs/content/. Unless otherwise specified, files are located at the same relative location as the url, in our case docs/content/overview/quickstart.md.\nChange and save this file.. Notice what happened in your terminal.\n\u0026gt; Change detected, rebuilding site \u0026gt; 29 pages created \u0026gt; 0 tags index created \u0026gt; in 26 ms Refresh the browser and observe that the typo is now fixed.\nNotice how quick that was. Try to refresh the site before it\u0026rsquo;s finished building. I double dare you. Having nearly instant feedback enables you to have your creativity flow without waiting for long builds.\nStep 4. Have fun # The best way to learn something is to play with it.\n"},{"id":4,"href":"/docs/compliance/ato-document-templates/","title":"Ato Document Templates","section":"Docs","content":"ATO Document Templates If you find yourself on a project where you might need an ATO, you should familiarize yourself with these documents. While every Agency has slightly different templates and workflows, using this will set you up for success.\nPrivacy Impact Assessment FIPS 199 Security Categorization Digital Identity Acceptance Statement Security Assessment Report Configuration Management Plan FIPS 199 Low Security Plan FIPS 199 Moderate Security Plan FIPS 199 High Security Plan\n"},{"id":5,"href":"/docs/develop/best-practices-for-submitting-pull-requests/","title":"Best Practices for Submitting Pull Requests","section":"Docs","content":" Best Practices for Submitting a Pull Request # Generally, a best practice to engage in before submitting your own pull request is to do a code review on your work.\nPrior to submitting a PR, review your code for the following characteristics:\nIs the code READABLE? Is it well-organized? Are the class, variable, and method names clear and descriptive? Are the formatting, spacing, comments, and indentations consistent? Have you refactored the code to reduce repetition and nested code? Have you included the appropriate comments to make your code readable to others? Have you followed the language’s STYLE CONVENTIONS? Are class, variable, and method names cased and capitalized according to the style conventions of the language (e.g. camelCase vs. snake_case) Is the code CORRECT? Does the code work as intended? Does the logic make sense and solve the intended problem? Have you provided instructions for reviewers to setup and verify the solution works? Have you identified and accounted for BUGS and EDGE CASES? Have you anticipated all the issues that could arise in your code that the person doing your code review will raise? Do you understand every line of code you are submitting? If not, go through each line and write down questions to clarify everything you do not understand, then ask! "},{"id":6,"href":"/docs/develop/bugs/","title":"Bugs","section":"Docs","content":" Defects/Bugs # Overview # How to manage bugs/defects. Tracking tools and integrating bug fixing into the regular development cycle.\nContents # Add some stuff here\n"},{"id":7,"href":"/docs/develop/code-reviews/","title":"Code Reviews","section":"Docs","content":" Code Reviews # Overview # Effective code reviews provide some of the best opportunities for learning, gaining valuable feedback from peers, ensuring that quality standards are upheld throughout a project, and defects are caught as early as possible.\nWe recognize code reviews can be a vulnerable exercise and should be approached with care and deliberate intention. We\u0026rsquo;ve collected some of the best practices for projects to systematize the code review process and lay out expectations for the creator of the pull request as well as the role of the reviewer.\nAlso worth reading is the pairing section because some of the same lessons overlap with code reviews even if it is happening informally or at a different stage.\n"},{"id":8,"href":"/docs/develop/configuration/","title":"Configuration","section":"Docs","content":" Configuration # Projects should adhere to the following guidelines for managing configuration:\nConfiguration values should be provided to programs via environment variables.\nPrograms should validate configuration immediately when they are run.\nIf config is incomplete or invalid, programs should:\nExit with a non-zero status Print a helpful error message to assist humans in resolving the issue All environment values should be cataloged in a single location.\nThis should generally be a project\u0026rsquo;s .envrc file (values can be left blank as needed), although ecosystems that have a strong convention for this configuration living elsewhere should follow that convention. The purpose of maintaining this catalog of environment variables is to document all possible configuration points. Projects should define default configuration for local development in an .envrc.local file. This file should be loaded from the project\u0026rsquo;s main .envrc file if it exists. Here is an example.\nFollowing the above guidelines should yield projects with a setup that allows developers to get a new project clone running with minimal manual configuration.\n"},{"id":9,"href":"/docs/develop/development-cycle/","title":"Development Cycle","section":"Docs","content":" Development Cycle # Overview # We practice agile with a small \u0026ldquo;a\u0026rdquo;. This means getting back to the basics of what it means to develop software with agility:\nFind out where you are Take a small step towards your goal Adjust your understanding based on what you learned Repeat In order to implement these principles, we do a few things:\nPlanning and executing tasks in repeated cycles, called Sprints At the end of each cycle, we review our work and whether or we accomplished what we set out to do. Please check with your Practice lead to learn which bug tracking tool is used to track your work.\n"},{"id":10,"href":"/docs/develop/docker/","title":"Docker","section":"Docs","content":" Docker # Overview # Docker is useful because it allows you to both package and run software with all its dependencies and configuration in isolation. It also allows you (in theory) to have the same environment in development as in CI and production.\nInstallation # You should install via homebrew\nbrew cask install docker Configuration of Shared Folders # Anecdata suggests that removing some of the default shared folders can decrease CPU usage. Usually you only need to share volumes under your home directory. Open the docker for mac preferences, then select \u0026ldquo;File Sharing\u0026rdquo; and then remove /Volumes and /private. You may also find improvements removing /Users and replacing it with /Users/YOUR_USERNAME_GOES_HERE.\nConfiguration of Storage Driver # If you\u0026rsquo;ve been running Docker for Mac for some time, make sure you are using the overlay2 storage driver.\n$ docker info |grep Storage Storage Driver: overlay2 If you don\u0026rsquo;t see overlay2, upgrading to the latest version will add that support, but you would need to recreate all of your docker data to utilize it. The Reset bomb should do it.\nConfiguration of Disk Image # If you\u0026rsquo;ve been running Docker for Mac for some time, make sure you are using the raw disk image format and not qcow2. Open Docker for Mac preferences and select Disk. Make sure the image ends with Docker.raw.\nConfiguration of Resources # Because docker uses a Virtual Machine under the hood on macOS, you can choose the maximum number of resources it is allowed to use. In the preferences under Advanced, you can select how many CPUs to assign, how much memory to assign, and how much swap to give the VM.\nSee also Understanding memory usage in Docker Desktop on Mac by the docker developers.\nVolume Mount Performance # Synchronizing the data between the host (your Mac) and the container can be resource intensive and/or slow. Read about the performance tuning options to see if they might apply to the conditions in your project.\nInspecting the Docker Virtual Machine # If you are having performance problems or are just curious, you can run commands inside the Virtual Machine with\ndocker run --rm=true -it --privileged --pid=host \\ justincormack/nsenter1 /usr/bin/top Inspecting Container Resources # To get a top like report of what your containers are doing\ndocker stats Cleaning Up # To see where all your disk space is going:\ndocker system df To remove stopped containers, dangling images, the build cache and unused networks:\ndocker system prune See the docker system prune documentation for more options like pruning volumes\nDocker Configuration # You can configure all sorts of defaults in the docker config.json.\nKeybindings # If you dislike the default detach keybindings of CTRL-p CTRL-q, you can override it in config.json with something like:\n{ \u0026#34;detachKeys\u0026#34;: \u0026#34;ctrl-@,ctrl-[\u0026#34; } Credentials Store # You can store docker credentials in an external credentials store.\nOn macOS, the keychain will be used by default, but if you need/want to store shared credentials, pass can be used.\nAdditional Resources # If you would like to dig deeper into how Docker works, how it isolates code from the host machine, and related ramifications a good place to start are the articles listed below.\nOfficial Docker Getiting Started Guide Linux Journal Weekend Reading Containers Particularly the articles Everything You Need to Know about Linux Containers Part I and II "},{"id":11,"href":"/docs/develop/editors-ides-debuggers/","title":"Editors Ides Debuggers","section":"Docs","content":" Editors, IDEs and Debuggers # Overview # We don\u0026rsquo;t care which editor/IDE/debugger that you use - it\u0026rsquo;s your workflow. That said, we do think there is enormous value to be had from a well configured working environment that leverages available tooling. For example, knowing how to use some form of debugger will significantly improve your productivity and reduce the daily frustrations of developing code.\nIn this section you will find overviews of the range of tools to use and suggestions on how best to configure each tool for the situations we most often use them.\nEditors # List of editors that engineers use (sorted alphabetically):\nAtom Emacs Goland Sublime Text vi(m) VS Code WebStorm Sublime Text # Plugins PackageControl Sublime Language Server EditorConfig JsPrettier (you will need to configure it to auto-format on save) Babel (for JSX syntax\u0026ndash;though looking for better option) Git vi(m) # VimAwesome.com provides a list of vim plugins sorted by how many GitHub dotfiles repositories have integrated the plugin. VimCasts short, bite-sized lessons on vim Plugins -vim-ale - Asynchronous code linter editorconfig vim-fugitive - A Git wrapper so awesome, it should be illegal vim-go - golang development plugin for vim vim-javascript vim-plug - Minimalist plugin manager vim-prettier Recover - Displays a diff before recovering a swap file VS Code # Plugins Prettier Path Intellisence "},{"id":12,"href":"/docs/develop/great-code-brings-happiness/","title":"Great Code Brings Happiness","section":"Docs","content":" Great Code Reviews Bring Happiness to my Heart # So you\u0026rsquo;ve put together the perfect changeset. You use your favorite tool to share your changes and ask politely for a code review.\nHow do you make that code review great? # Making code review process work for your team We\u0026rsquo;ve all been on teams large and small that have tried to integrate code review into their process. Some of those teams succeeded, and others failed. There were several themes that arose that were indicators for success or failure. Here are the practices we\u0026rsquo;ve found that tend to make code reviews a success.\nDo it asynchronously. If you or the reviewer are at all busy, and you probably are, do it asynchronously. Let the reviewer know there’s an item awaiting their attention, then give them a day to get to it. If they don’t get to it in a day, bug them again.\nUse a visual code review management tool. Use a code review tool which keeps track of comments on changes, and allows in-line discussion. In addition to GitHub’s code review tool there is Google\u0026rsquo;s Gerrit, Phabricator that started as a Facebook project, or JetBrains Upsource as a paid option.\nMake code review a gatekeeper step in your source management process. If it’s something that must pass before code can be merged, then it will happen. GitHub’s pull request feature does a great job of supporting an integrated code review step. A dashboard shows what pull requests are outstanding, and a merge button minimizes friction once the changes have passed review. If you try to add code review as an adjunct step to an existing process - e.g. sending an email asking for a code review, with no central dashboard where outstanding review requests can be seen - it can be very difficult to consistently employ code review, since its existence will be somewhat invisible. YMMV.\nIntegrate your tool into a team communication medium. Have the code review tool notify you when relevant changes have been made or commented upon. Email, chat, carrier pigeon, or tweet - whatever medium will catch your attention in a timely manner, and not break your flow.\nPerforming the review # Once you’ve done the above, there are several additional things each person can do to make code reviews more effective on an individual basis.\nWhen asking for a code review, try to find a trusted critic. The code review step in a revision control workflow process is like this magical land where you can go with code and make it pretty before the ball, so that when it walks through that door it will shine like Cinderella on her big night out. You want the code reviewer to give you everything they’ve got. You may decide that some suggested changes are not actually necessary. However on the first code review pass, you want all the criticisms you can get. The more detailed the better, within limits.\nThe code reviewer should be looking at many aspects of the code at once. The overall structure, the structure of individual methods or classes, the syntax, the nomenclature, the comments, the documentation, the testability, and the adherence to style guides used for your project. (If you don’t have a style guide, the existing style is your guide. If there isn’t an existing style, you should scrap what you’ve got and start over. \u0026hellip;just kidding. In this last case, you should probably pick the style you think is best, and adhere to that.) Checklists can help to make sure you don’t miss anything here.\nThe commenters in a code review should start by trying to understand the motivation of the person writing the code. If that is not clear to you, then a good comment might be one inquiring as to the purpose of the code. Once you understand the coder’s motivation, use this perspective as a jumping off point to provide observations and suggestions that might make the code better. When critiquing someone else, starting from a perspective of understanding will help you to make a more informative and effective critique.\nBe clear, succinct, and courteous. This is the best way to make sure your critiques/comments are heard and understood. Give reasons to back up your suggestions. Cite style guides where they exist. Leave room for the other person to explain where you might be missing something.\nAfter the first round of comments, the person who requested the code review should review the comments and make changes or give feedback. Where in doubt, defer to the opinion of the reviewer. You can always change it later!\nComplete the code review soon after it was requested. Under 15 minutes is fantastic, under a day is great. Under a week can be fine depending on how the changes affect the project. Any longer than a week, and the changes begin to get stale. Not just in the codebase, but also in your head. The fresher you can be when reviewing the changes, the faster all the steps of the code review process will be. If the code begins to get old and stale, escalate the review of those changes in priority, or question whether they are important enough to matter in the first place.\nAs a general comment on style, everything goes faster if pull requests are of a reasonable size. If the reviewer can grok your changes in under 5 minutes, you\u0026rsquo;re golden. Sometimes a larger commit is necessary, and that\u0026rsquo;s fine.\nHowever commits that take longer than 10-15 minutes to review should be questioned, or framed in a way that helps the reviewer understand why the commit must be so large. Larger commits slow project velocity due to this code review friction, and are to be avoided if possible.\nIBM did some nifty research in this area to see how code review size affects code quality. Notably, review of one’s own code prior to submission was shown to positively affect code quality as well.\nWhere there are conflicts, bring in more reviewers and resolve outstanding questions. Don’t become attached to your code. Defer to the majority. And remember it can always change again later. ;)\nAnd enjoy! Code review is one of the best opportunities to learn that you have on a daily basis. Take what you learn in code reviews and use that to improve your coding habits daily.\nHappy coding!\n"},{"id":13,"href":"/docs/develop/growth/","title":"Growth","section":"Docs","content":" Growth # Overview # When working on a project, it\u0026rsquo;s important to allow time for career growth. Folks will have different levels of experience and will need to explore areas that are not directly related to completing project work.\nWe experience that this exploration winds up benefiting the client in the long run, both through immediate impact (e.g. speeding up tests) and the client getting the benefit of learning that happened elsewhere. Allowing time for growth ensures consistent, high quality work which benefits everyone.\nMentoring is an important part of growth. We will be sharing some techniques for mentoring.\nTo ensure that growth happens, the time should be built into the estimate for a ticket. From time to time, it will be appropriate to restrict exploration and \u0026ldquo;just get the ticket done\u0026rdquo;, but that should happen infrequently.\nIf someone is interested in a topic that is not related to their client work, a separate plan will need to be made. That might involve changing client teams, changing client projects, or joining Rhino Reserve.\n"},{"id":14,"href":"/docs/develop/health-checks/","title":"Health Checks","section":"Docs","content":" Health Checks # Overview # In modern API services, it is not uncommon to build and rely upon Healthcheck endpoint(s). These endpoints are generally orthogonal to the operating business logic of the service, and are intended to be consumed by the Operators of the system (e.g. InfraSec practitioners).\nThere are many ways that Healthcheck endpoints can be used in a system:\nReadiness - Upon startup, has my app warmed up any internal caches that are needed before it can respond to traffic? Liveness - Is this app accepting and responding to traffic at all? Versioning - Is the verison of code that is deployed what it is expected to be? Reachability - Can a client reach the service over the network? Catalog - What are the external dependencies to which this app connects? Connectivity - Can the app connect to its external dependencies successfully? Quality - Does this endpoint do all its work and respond in a timely manner? Many apps will try to achieve their subset of the above goals via a single endpoint, but as apps grow in sophistication it is not unheard of to tease those responsibilities out to separate endpoints.\nSecurity # Frequently healthcheck endpoints are agnostic about authentication and authorization, meaning anyone that can reach the service via the network can access these endpoints.\nHaving heathcheck endpoints be un-authenticated implies several other considerations:\nAny information available in the healthcheck should not be confidential, nor should the information be able to be (ab)used to jeopardize the smooth running of the service. Unsecured healthchecks could open up the system or its checked dependencies to Denial of Service attacks. This may be mitigated by adding some level of caching in the healthchecks. One possible technique for lowering risk is to isolate operational endpoints to a separate port with tighter network ACLs. Reliability # Particular care should be taken when using healthchecks as part of the reliability story of an app.\nProf von Neuman is credited with the concept of \u0026ldquo;synthesis of reliable organisms from unreliable components\u0026rdquo;. He showed that the math works out as such: if my app is serially dependent on 3 other services that are each at 97% reliable, my app can be no more than ~91% (0.97 x 0.97 x 0.97) reliable. This means that if an app\u0026rsquo;s healthcheck is setup to fail when any of its dependencies have failed, this has significantly limited the upper bound of the apps reliability.\nSometimes an app\u0026rsquo;s reliability really is limited by a dependency, e.g. a database connection. If every single endpoint and functionality in an app requires interaction with the app\u0026rsquo;s database, then it is reasonable to call your app fully inoperable when the database connection is not functioning.\nHowever, if only a subset of the app\u0026rsquo;s functionality is impacted by an unreliabile dependency, it may be better to handle that instability at runtime, rather than declaring the whole app down. Consider the following example: if only one (out of many) of an app\u0026rsquo;s functionalities depend on sending email, the app should maybe not be considered wholly down if the email service is unreachable; it would be better to handle errors from interacting with the email service gracefully, or possibly even using a technique like feature flagging to turn off any attempts to use that functionality until the service is back in service.\nHealthchecks in Deployment Pipelines # It is not uncommon to see teams utilize an app\u0026rsquo;s healthcheck endpoint on the critical path of a deployment pipeline, however this should be evaluated for unintended consequences.\nIn modern software engineering, automated deployments are a critical feature of a software system, facilitating the ability to safely and incrementally improve the quality of a running app.\nAgain, consider the app example from above that uses an email sending service for one feature: should the external service instability be able to jeopardize the ability to roll out new code? Perhaps the new code for deployment is useful for diagnosing the issue or is intended to temporarily turn off the offending sending of emails: blocking roll out makes remediation of the issue harder rather than safer.\nContents # von Neuman paper Reliability Engineering "},{"id":15,"href":"/docs/develop/intro-to-ci-cd/","title":"Intro to Ci Cd","section":"Docs","content":" Intro to CI/CD # The goal of CI/CD practice is to provide a workflow that can support frequent updates, good testing, consistent builds, and prompt deploys. Additionally issues with code should be found quickly and addressed before it is released to customers.\nDelivery Pipeline Basics # There are usually four conceptual steps in a delivery pipeline:\nBuild: Where you build the code into a binary or other artifacts to eventually distribute. Test: Where you validate or test the artifacts built in the previous step. Deploy: Where you configure and deploy the artifacts to an environment. Could be a pre-prod or prod. Release: Where you finally allow users access to that version of code you\u0026rsquo;ve configured and built. In many cases you can merge or swap the steps Release and Deploy.\nPhilosophical ideas and notes about each step are outlined in the following sections. These are not the be-all end-all of what we do, but are a starting point for how we think about this process.\nNote: These steps are all fungible. They can be combined to a degree, you can have multiple deployment and validation steps. You could merge deploy and release. What is important is that each of those steps is well understood and documented. That they are configurable and repeatable.\nBuild # Builds should be repeatable. # This means you should be able to check out the code from your project at that same commit hash and build it again and get the same artifact(s). Note: This means that dependencies should be versioned in your codebase. To keep dependencies up to date in an automated way you can use Dependabot with our documentation on configuring it. Please do whatever is \u0026ldquo;correct\u0026rdquo; in the context of the languages/frameworks your project is built in.\nBuilds should be hermetic. # This means that the build should be isolated from other builds. In CI, the build shouldn\u0026rsquo;t share the same workspace or files from a previous build or from a build of a different project.\nBuilds should output an immutable artifact. # Artifact outputs should not be rewritten or altered by subsequent builds. This allows you to distribute or redeploy from these unaltered artifacts for different points in code. Additionally, try to label the artifact with the appropriate versioning scheme for your project.\nNotes about Versioning # Version your code and artifacts. Doesn\u0026rsquo;t really matter how, just do it. It will make it easier to track down issues or deploy specific versions of your project.\nIn your code consider adding a command or an endpoint that surfaces that version for debugging.\nTag your versioned releases on your mainline branch. This helps you find the state of mainline at the point in time your artifact was built. Additionally, if you are using GitHub you can use their releases functionality to share release artifacts and notes.\nHere is a preferential ordering of versioning schemes:\nSemantic Versioning # Why semver? It tells you and your customer how much has changed since the last released code and sets expectations accordingly. If you are tagging at mainline where you are building from, you can rebuild the artifact from the same point.\nFrom semver.org:\nGiven a version number MAJOR.MINOR.PATCH, increment the: MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format. Commit Hash # A commit hash is unique (with extremely few collisions) and is easily linked back to history in code. However, how much has changed is opaque to your users and it is difficult to determine how old this version is in comparison to other versions.\nOther versioning strategies # These are other versioning strategies we\u0026rsquo;ve seen. We do not recommend them.\nFeature branch related names. These should be short lived and maintained for no more than a few days. Most users will not find these useful. Build id related names. These are opaque to a user and harder to dig up history on when debugging. Build date related names. These are also opaque to a user and difficult to dig up history on when debugging. You at least get a sense of when these changes went in but are hard to tie to a commit in mainline. Test # We will not be writing deeply about testing methodologies or specifics around kinds of tests and the philosophies here.\nIn the context of a CI pipeline:\nTests should be hermetic. # Ideally, multiple CI runs should be able to run at the same time without affecting each other.\nTests should be idempotent. # Tests in CI should be able to run several times over without producing inconsistent effects.\nTests should be reproducible in a developer\u0026rsquo;s computer. # So developers can debug them more easily.\nTests should run fast. # \u0026ldquo;Fast\u0026rdquo; is poorly defined here. Generally speaking, you want tests to be fast so merging code changes is fast. A developer shouldn\u0026rsquo;t think they can start on a new piece of work in the hour or two to push a change through CI and context switch.\nTests should fail fast. # Similar to the previous point, \u0026ldquo;fast\u0026rdquo; is relative but in CI you will want to give the waiting devloper signal on whether their work can be merged or requires changes as quick as possible.\nDeployment # Deployment will differ project to project and should take into account the requirements of the system being deployed to.\nDeployment should be repeatable. # The same code and configuration should be deployable again and again.\nDeployment process for software should be the same regardless of environment. # Using the same process and tooling means lower cognitive overhead in deployment and your non production environments should reflect production more closely.\nDeployment tools should be maintained separate from the application being deployed. # We\u0026rsquo;ve seen this happen where we need to fix some deployed code but also had a problem with the code that performs the deployment and all of it is a hairy mess of debugging.\nRelease # There are a few different ways of implementing release. No matter what, release is the step when you allow customers to have access to that particular version of your code.\nIf you\u0026rsquo;re releasing a CLI, it may mean that you\u0026rsquo;re releasing your binary to a Homebrew tap or just a zip file to GitHub releases.\nIf you have a service, you could have a loadbalancer manage traffic between different versions of your services. Or you could use something like Launch Darkly to leverage feature flags to do something similar for different user cohorts.\nContinuous (Integration | Delivery | Deployment) # We throw around terms like CI or CD with the assumption that we know what the differences are between these practices.\nFor clarity, we\u0026rsquo;ll refer to them as follows:\nContinuous Integration # Continuous Integration is when you validate on every push to mainline. This helps folks find failures in the code before it reaches mainline. Ideally, automated build and test steps validate changes and stop any new changes to mainline. Mainline must always be in a good state to deploy from.\nThis may manifest as a process where developers merge changes into mainline as often as possible. Small incremental changes to code and small discrete tests make changes easier to understand and facilitate frequent merges in CI. This practice helps mitigate risks from long lived branches and helps validate small changes in quick succession.\nThat being said, in practice we see longer lived branches for features or versions. These branches need to be kept up to date with mainline with frequent merges or rebases from mainline.\nThe point of either of those practices is to minimize drift between your working branch and mainline so validation in CI or manually is easier.\nContinuous Delivery # Continuous Delivery can be described as an extension to Continuous Integration. In addition to automated build and test steps running on every proposed change to mainline, the delivery of the code is also automated.\nYou want the software built to be deployable at any given point in time.\nDelivery may mean different things depending on your workflow. It may mean that you have built your artifacts and validated them and delivered them to a centralized repository. For example, Docker containers might go to Docker Hub or Amazon Elastic Container Registry (ECR). Might mean that the code has been deployed to a staging environment. All of these could be \u0026ldquo;delivered\u0026rdquo; depending on the project\u0026rsquo;s workflow.\nContinuous Deployment # Continuous Deployment can be described as a further extension of Continuous Delivery. In addition to automated build, test, and delivery, production deployment is also automated.\nThis is an advanced state that requires excellent, trustworthy, automated tests and monitoring. The complexity of continuous deployment system is correlated with the complexity of the delivery pipeline. It\u0026rsquo;s much easier to have continous deployment of a single container image than a full-featured web application.\n"},{"id":16,"href":"/docs/develop/programming-languages-golang/","title":"Programming Languages Golang","section":"Docs","content":" Go # Overview # Go (a.k.a. Golang) is a programming language designed by folks at Google.\nThis guide provides some general resources for working in Go. Web Application in Go provides some specifics to web development.\nLearning Resources # References # The Go Programming Language Official Website Effective Go (how to do things “the Go way”) Go Pointer Primer pkg.go.dev (where you can read the docs for nearly any Go package) Go wiki Book: The Go Programming Language Advanced Testing with Go Video and Article (great overview of useful techniques, useful for all Go programmers) Go Proverbs Line of Sight Go Go for Industrial Programming Tours/Lessons # A Tour of Go (in-browser interactive language tutorial) How to Write Go Code (info about the Go environment, testing, etc.) Go by Example Go Track on Exercism Article: Copying data from S3 to EBS 30x faster using Golang Testing # General # Use table-driven tests where appropriate. Make judicious use of helper functions so that the intent of a test is not lost in a sea of error checking and boilerplate. Comments delineating the 3 or 4 phases of your tests can help with comprehension. Use t.Helper() in your test helper functions to keep stack traces clean. Use t.Parallel() to speed up tests. Trend away from using testify/suite. (It used to address some shortcomings in the standard library testing tools that have since been addressed.) Lightweight assertion packages can help with expressiveness. Consider using testify/assert or is Coverage # Always test exported functions. Exported functions should be treated as an API layer for other packages. Cover the expected behavior and error scenarios as a user of that API. Consider using the _test package suffix to simulate calling the package under test from an external package Try not to test unexported functions. Unexported functions are implementation details of exported ones and should not change the intended usage. If you find that an unexported function is complex and needs testing, it might mean it needs to be refactored as it\u0026rsquo;s exported function elsewhere. Context # Function First Parameter # As a general rule, context.Context should be passed down through the layers of your program, as this is the conventional Go way to address \u0026ldquo;cross cutting concerns\u0026rdquo;, e.g. cancellation, logging, distributed tracing, or other types of instrumentation (which in other languages might be addressed via Thread Locals or similar constructs).\nAt Google, they found this pattern to be so useful that they require it.\nErrors # FYI, Go\u0026rsquo;s error handling had a bit of a shift and update as of 1.13 in ~2019.\nWrap Errors # Prior to Go 1.13, people were forced to make a hard choice: either add descriptive context around an error that was received, or to just pass the error through in case it was a \u0026ldquo;sentinel error\u0026rdquo; like the canonical io.EOF example. Nowadays you can both add context to an error, and still ensure the caller can examine the error to see if it is a sentinel.\nerr := ... if err != nil { if oldStyle == true { // this is the pre-1.13 error encapsulation. // the problem here is that this created a whole new error, // which hid the underlying error type. return fmt.Errorf(\u0026#34;\u0026lt;richer contextual information\u0026gt;: %v\u0026#34;, err) } // using the \u0026#34;%w\u0026#34; (wrap) verb means that the error chain is returned // and can be examined by the caller using the // `errors.Unrwap(...)` or `errors.Is(...)` functions return fmt.Errorf(\u0026#34;\u0026lt;richer contextual information\u0026gt;: %w\u0026#34;, err) } Since these capabilities changed fairly recently, it\u0026rsquo;s common to see many libraries and legacy code not using the wrapping pattern, but going forward it is preferable to opt for wrapping errors where possible.\nHandle Errors Once # This advice is mostly stolen from the second-to-last section of Dave Cheney\u0026rsquo;s somewhat dated blog post.\nIf there are 0 side effects from an error received in your code, this means you are swallowing it, and this will generally make someone\u0026rsquo;s life (maybe even yours) more difficult in the future. Please don\u0026rsquo;t ignore errors.\nIf there are 2+ side effects from receiving an error, you are adding too much noise to the system. This usually manifests as doing BOTH logging of the error AND returning the error back up the call stack, but if every layer were to do this you\u0026rsquo;d fill your logs with unnecessary duplication.\nEliminate Errors # This section is ispired by another Dave Cheney missive.\nIt is possible to reduce error handling overhead by considering if your functions absolutely need to return errors. Consider the following:\nfunc Logger(ctx context.Context) (*zap.Logger, bool) { logger, ok := ctx.Value(loggerKey).(*zap.Logger) return logger, ok } This function means that every place that wants a *zap.Logger has to deal with the !ok condition, creating a lot of boilerplate checking at callsites who likely have no idea what the proper fallback is if there is no logger defined.\nAs a consumer, wouldn\u0026rsquo;t the following function signature be easier to deal with?:\nfunc Logger(ctx context.Context) *zap.Logger { logger, ok := ctx.Value(loggerKey).(*zap.Logger) if ok { return logger } fmt.Fprintf(os.Stderr, \u0026#34;no logger configured\u0026#34;) return zap.NewNoopLogger() } Callers can now be sure that they will always get a logger of some type. And we\u0026rsquo;ve centralized, rather than distributed, the fallback behavior. (This implementation chose to communicate the configuration problem to the system operators via stderr, but you might also choose to panic(), or do something else, YMMV.)\nPackages # Dependency Management # Go Modules has become the standard way to manage your dependencies Before Go Modules were solidified, dep was frequently used for dependency management. This might be helpful if you are dealing with an older project: Daily Dep documentation (common tasks you’ll encounter with the dependency manager) Prefer Standard Libraries # In general, when selecting new packages, highly consider standard libraries over third party dependencies. One of the strengths of Go is its core packages, such as http, json, and sql. These libraries also use vocabulary and patterns easily accessible via popular public Go resources, which are often translatable to modern programming approaches in their respective areas. This creates an easier bridge for Engineers new to Go or domain areas (such as relational databases) to adjust and onboard.\nSince the third party ecosystem is still new, packages may have little community support, follow opinionated patterns inconsistent with Go idioms, or lack long term support. When choosing a third party package, carefully weigh the cost adoption and support contributions. Document the decision and any shortcomings of a comparable standard library along with a rollback plan.\nThird Party Packages # Some examples of third party packages we\u0026rsquo;ve found to be helpful and stable are:\nsqlx for SQL querying and struct marshalling. cobra/pflag/viper for writing command line utilities. If you\u0026rsquo;re exploring a new package, Awesome Go is a good place to start.\nTime # Clock Dependency # time.Now() can cause a lot of side effects in a codebase. One example is that you can\u0026rsquo;t test the \u0026ldquo;current\u0026rdquo; time that happened in a function you called in the past\nFor example, let\u0026rsquo;s say we have the following:\npackage mypackage import \u0026#34;time\u0026#34; func MyTimeFunc() time.Time { return time.Now() } func TestMyTimeFunc(t *testing.T) { if MyTimeFunc() != time.Now() { // This will error! // The time in the function and the test happen at different times t.Errorf(\u0026#34;time was not now\u0026#34;) } } How do we test the contents of the return here? If we want to assert the time we need a way to know what time.Now() was when the function was called.\nInstead of directly using the time package, we can pass a clock as a dependency and call .Now() on that. Then in our tests, we can assert against that clock! The clock can be anything as long as it adheres to the clock.Clock interface as defined in the facebookgo clock package. We could, for example, make the clock always return the year 0, or the 2019 New Year, or maybe your birthday! In this clock package, there are two clocks.\nThe real clock where clock.Now() will call time.Now(). A mock clock where clock.Now() always returns epoch time. We\u0026rsquo;ll show later how to change that! Let\u0026rsquo;s look at the example above with the clock package.\npackage mypackage import \u0026#34;fmt\u0026#34; import \u0026#34;time\u0026#34; import \u0026#34;github.com/facebookgo/clock\u0026#34; func MyTimeFunc(clock clock.Clock) time.Time { return clock.Now() } // Then our caller func main() { // clock.New() creates a clock that uses the time package // it will output current time when .Now() is called fmt.Print(MyTimeFunc(clock.New())) } Then in our tests we can use a mock clock that freezes .Now() at epoch time:\nfunc TestMyTimeFunc(t *testing.T) { testClock := clock.NewMock() if MyTimeFunc(testClock) != testClock.Now() { // both should equal epoch time, we won\u0026#39;t hit this error t.Errorf(\u0026#34;time was not now\u0026#34;) } } Cool, but what if I want to use a different date? Say my test relies on our TestYear constant. The clock.Mock clock allows us to add durations to the clock and set the current time. Note that the clock.Clock interface does not allow this, it needs to happen before passing the mock clock through the interface parameter.\nSetting the Mock Clock # Here\u0026rsquo;s an example using the test above and setting the time to September 30 of TestYear:\nfunc TestMyTimeFunc(t *testing.T) { testClock := clock.NewMock() dateToTest := time.Date(TestYear, time.September, 30, 0, 0, 0, 0, time.UTC) timeDiff := dateToTest.Sub(c.Now()) testClock.Add(timeDiff) if MyTimeFunc(testClock) != testClock.Now() { // both will now be September 30 of TestYear // we\u0026#39;ll pass the test again t.Errorf(\u0026#34;time was not now\u0026#34;) } } http.ResponseWriter # HTTP Status Codes \u0026amp; Bytes Written # There are a surprising number of footguns associated with trying to gather statistics about an HTTP request, having to do with several obscure and optional (\u0026ldquo;smuggled\u0026rdquo;?) http.ResponseWriter interfaces. This is a regularly occurring challenge for anyone trying to implement a request- logging middleware.\nThe recommended package for this task is called httpsnoop, and the author has a great explaination of \u0026ldquo;Why this package exists\u0026rdquo;.\n"},{"id":17,"href":"/docs/develop/programming-languages-python-installation/","title":"Programming Languages Python Installation","section":"Docs","content":" Installation # Python # Use Python 3.\nUse brew in order to keep Python up to date. Install the latest version like so:\nbrew install python Pipenv # If you also need a local Python development environment, we recommend using pipenv to provide a virtual environment. Install it using homebrew:\nbrew install pipenv To create a local development environment and a Pipfile, specify the specific python interpreter to use:\n$ pipenv --python 3.7.6 Creating a virtualenv for this project… Pipfile: /Users/jimb/code/Engineering-Playbook/Pipfile Using /usr/local/opt/python/libexec/bin/python (3.7.6) to create virtualenv… ⠹ Creating virtual environment...Already using interpreter /usr/local/opt/python/bin/python3.7 Using base prefix \u0026#39;/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7\u0026#39; New python executable in /Users/jimb/.local/share/virtualenvs/Engineering-Playbook-7M1kEMMv/bin/python3.7 Also creating executable in /Users/jimb/.local/share/virtualenvs/Engineering-Playbook-7M1kEMMv/bin/python Installing setuptools, pip, wheel... done. Running virtualenv with interpreter /usr/local/opt/python/libexec/bin/python ✔ Successfully created virtual environment! To work within the development environment, use pipenv shell.\nFurther Reading: Example Pipenv Workflow\nTo install additional dependencies for the project, use pipenv and not pip:\npipenv install flask pipenv install pytest --dev # install a dependency only needed for development Further Reading: Environment Management with Pipenv\n"},{"id":18,"href":"/docs/develop/programming-languages-python-linters-checkers/","title":"Programming Languages Python Linters Checkers","section":"Docs","content":" Linters and Checkers # We recommend using flake8 as a linter and black for formatting.\n"},{"id":19,"href":"/docs/develop/programming-languages-python-project-setup/","title":"Programming Languages Python Project Setup","section":"Docs","content":" Project Setup # Layout # These recommendations are based on this article, updated to reflect the use of Pipenv.\nSimple project # .gitignore README.md LICENSE Pipfile Pipfile.lock sample.py # module lives in a single file tests/test_sample.py Application or larger project # .gitignore README.md LICENSE Pipfile Pipfile.lock sample/__init__.py sample/*.py # module lives in a directory docs/ tests/test_*.py Library # Packaging a python library\n.gitignore # We suggest copying this Python-specific .gitignore into the root of your project:\ncurl https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore -o .gitignore "},{"id":20,"href":"/docs/develop/programming-languages-python/","title":"Programming Languages Python","section":"Docs","content":" Overview # Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python\u0026rsquo;s design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\nContents # Installation Linters and Checkers Project Setup "},{"id":21,"href":"/docs/develop/programming-languages-shell/","title":"Programming Languages Shell","section":"Docs","content":" Shell programming # Overview # Best practices tl;dr:\nThink hard before using bash. Something else is often better. For portability, dash provides a minimal POSIX shell feature set. For complicated tasks, a more modern language like Go or Python may be more reliable and easy to maintain. Use shellcheck Don\u0026rsquo;t copy \u0026amp; paste code from Google or StackExchange unless you fully understand it Keep your bash version up to date with your package manager (e.g. brew install bash and routinely running brew upgrade), or stick to whatever version your team has agreed to standardize on (e.g. brew pin bash). However, be aware that behavior of bash code can vary slightly between different versions of the interpreter. Consolidate your bash profiles into a single ~/.bashrc file to streamline machine instructions affecting your PATH environment variables. Debugging # Bash will emit debugging output after set -x is executed. set +x to disable debugging output. bash -x ./brokenscript has the same effect without requiring edits to the file. It is also possible to configure the format of the output:\nexport PS4=\u0026#39;+$BASH_SOURCE:$LINENO:$FUNCNAME: \u0026#39; For extra verbose debugging, it may be useful to combine this option with set -v: this will also display the shell lines as they are read (i.e. before substitutions are resolved, etc).\nTools # shellcheck is an excellent code linter for bash that catches many common anti-patterns. Integrations with several popular editors are available. If you only take one thing from this document, make it \u0026ldquo;use shellcheck\u0026rdquo;. bashdb is an interactive bash debugger inspired by gdb. Pitfalls # Shell programming is deceptively perilous (see Bash Pitfalls on Greg\u0026rsquo;s Wiki and Beginner Mistakes on Bash-Hackers Wiki). Sites like tldp.org and StackOverflow have high pagerank for shell programming questions, but code blocks posted on these sites will not uncommonly have smells or dangerous errors. Check the Resources section of this document for some more trustworthy sources.\nPOSIX is very permissive about what can go in a file name. In fact, anything except a null byte could be in a filename. Problematic characters include (but are by no means limited to) tabs, newlines, non-breaking spaces, glob characters, and leading dashes. Many beginner mistakes are a result of failure to fully grok the implications of this; e.g. word splitting on whitespace, or globs getting expanded in surprising ways. Aggressively quoting substitutions without a specific reason not to is good practice. bash is ubiquitous, but bash != bash. Not infrequently, a version of bash will change its handling of a particular case, only to revert to the old behavior in the next version. MacOS ships bash 3.2.57, but as of this writing bash 5.0 is in homebrew. Other versions may be on whatever host your script will run on, but there is no portable way to force which bash version you will get. On many systems, /bin/sh resolves to bash in POSIX compatibility mode, but you can\u0026rsquo;t be guaranteed of that: prefer being explicit and using #!/usr/bin/env bash for the shebang. Be aware of the unpredictable behavior of set -e / set -o errexit. This is useful when developing a script, but may not behave as you expect. The behavior is unpredictable depending on which shell version your script ultimately runs on; based on this history, consider that it may even change further in future versions of bash. Avoid using cd in the main thread when possible, and always include error handling in case the command fails. Losing track of this state can lead to disastrous consequences. Try to use absolute paths, or a subshell like ( cd somedir || exit ; somecommand ). Don\u0026rsquo;t iterate through lines in a stream with for. Prefer [[ to [: [[ is a bash extension that is a strict upgrade over [. PATH # PATH is a shortcut to a certain location on your machine, which you can view by typing echo $PATH in the terminal. To oversimplify, a PATH is an environment variable which consists of a list of directories. Adding a PATH to your bash profile will give your machine direct instructions on where to look for a certain program.\nOne of several files can affect how your machine searches for a given PATH, listed in the order a Mac searches for them:\n~/.bashrc (read on every new shell, so looked at far more often) ~/.bash_profile (read only on login) ~/.profile If you have multiple files affecting your PATH, consider consolidating them into one /.bashrc and deleting the others. Next, organize each PATH directory in your ~/.bashrc from most-used to least-used to maximize efficiency.\nResources # The bash man page: man bash GNU Bash Manual Bash Guide Bash FAQ Bash Pitfalls Bash-Hackers Wiki Path Variables CLI Navigation Shortcuts Notes # The Bash CLI navigation shortcuts resource linked above refers to a keyboard shortcut for moving the cursor from word to word: meta key + f or b (under section 8.2.2 Readline Movement Commands). This shortcut does not work out of the box on a Mac. In the macOS Terminal app and iTerm2, the default keyboard shortcut for moving from word to word is option + left or right arrow. If you want to use option + f or b, there is a checkbox in Terminal app to Use Option as Meta key under Preferences -\u0026gt; Profiles -\u0026gt; Keyboard. In iTerm2, you can change key mappings under Preferences -\u0026gt; Profiles -\u0026gt; Keys.\n"},{"id":22,"href":"/docs/develop/programming-languages/","title":"Programming Languages","section":"Docs","content":" Programming Languages # Overview # This section will contain our collective observations about the strengths and challenges offered up by many of the different programming languages out there.\nWhile we may have opinions on particular languages and strategic reasons for investing in some languages over others, we recognize that our clients will may make different choices for different reasons. As such, Rhinos should aim to be proficient in more than one language and able to get by with several.\nLanguages Rhino\u0026rsquo;s are proficient in # Go JavaScript (including TypeScript/FlowScript) Swift Rust Kotlin Ruby Python Java C/C++ bash SQL Language Resources # Cross-Language Patterns # In many projects, it is fairly common to have interoperation between disparate languages, e.g. a Python codebase that also builds an OpenAPI spec or Go entities that are marshaled into JSON.\nAs a general guideline, it is recommended to use identifiers that are idiomatic to the destination representation. For example, prefer using camelCase with JSON APIs.\nSome things to consider:\nSwagger/OpenAPI specs are intended to be JSON (or YAML), so whether you\u0026rsquo;re coming from Python (snake_case) or BASIC (CAPITAL_CASE) or Go (PascalCase), prefer to output JSON with idiomatic camelCase identifiers. Structured logging usually uses JSON as the destination structure, so when creating log attributes/names, prefer JSON idiomatic identifiers in camelCase where possible. Go\u0026rsquo;s idiomatic \u0026ldquo;acronyms should be capitalized\u0026rdquo; is actually lossy when it comes to acronym/word boundaries, so carefully consider where you apply that capitalization rule. For example, some code generators might turn the identifier ExampleHTTPID into a filename that looks like example_h_t_t_p_i_d.txt, but if you use ExampleHttpId the generator has a much better chance of spitting out something reasonable like example_http_id.txt. JavaScript # Important JS patterns and features to understand:\nDestructuring Assignment A Dead Simple Intro to Destructuring JavaScript Objects Fat Arrow Functions ES5 Functions vs ES6 Fat Arrow Functions Higher Order Components Higher Order Components: A React Application Design Pattern Promises An incremental tutorial on promises Spread Operator/Rest Params JavaScript \u0026amp; The Spread Operator How Three Dots Changed JavaScript Template Literals Template Literals React and Typescript Rhino resources for React, Typescript, and front end tooling SQL # Query Log Observability # Did you know that SQL dialects have provisions for leaving comments within their queries? The following observability hint comes directly from Database Reliability Engineering:\nWhen doing SQL tuning, a big challenge is mapping SQL running in the database to the specific place in the codebase from which it is being called. In many database engines, you can add comments for information. These comments will show up in the database query logs. This is a great place to insert the codebase location.\nSome relevant docs:\nPostgres MySQL Additionally, if you have a request-scoped identifier (e.g. a requestID or traceID), you might be able to weave that into a built SQL query to give even finer-grained visibility.\nA quick example code snippet:\nfunc (s *Store) listSystems(ctx context.Context) ([]*models.System, error) { results := []*models.System{} comment := fmt.Sprintf(\u0026#34;/* storage/system.go:listSystems %s */\u0026#34;, appcontext.TraceID(ctx)) err := s.db.Select(\u0026amp;results, comment+sqlListSystems) if err != nil { appcontext.ZLogger(ctx).Error(\u0026#34;Failed to fetch systems\u0026#34;, zap.Error(err)) return nil, err } return results, nil } Results in query log:\n2021-02-09 16:59:18.686 UTC [456] LOG: statement: /* storage/system.go:listSystems 211956de-ebc3-4096-8496-329c622dba2a */ SELECT id, created_at, updated_at, project_name FROM system_intake WHERE status=\u0026#39;LCID_ISSUED\u0026#39; AND request_type=\u0026#39;NEW\u0026#39; AND lcid IS NOT NULL; "},{"id":23,"href":"/docs/develop/source-control-master-main/","title":"Source Control Master Main","section":"Docs","content":" Master to Main branch rename # These instructions will help moving repos from using the default branch name master and modify it to be main.\nPrereqs # First, ensure that the version of git that you have installed is at least v2.28.0. This is important to ensure you have access to the init.DefaultBranch setting in the git config file.\nInstructions to change default branch name # To update the branch name use the npm tool lgarron/main-branch:\nnpm install -g main-branch main-branch repo sample/master-to-main-test replace master main Follow up with local modifications:\ncd git-repo-name git pull origin main git checkout main git branch --delete master git branch --set-upstream-to=origin/main main git symbolic-ref HEAD refs/heads/main git symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/main git fetch --all --prune Test that the master branch has been removed by running this command and getting a shell error (exit code not 0):\ngit checkout master Future proofing # If you\u0026rsquo;re using git v2.28.0 or later you should be able to configure git globally to ensure main is now your default branch permanently.\ngit config --global init.defaultBranch main Now in your ~/.gitconfig file you\u0026rsquo;ll see this addition:\n[init] defaultBranch = main For older versions of the git tool you\u0026rsquo;ll have to be careful to not create new repos with master as the default branch. First make a new git alias:\ngit config --global alias.main \u0026#39;!git init \u0026amp;\u0026amp; git checkout -b main\u0026#39; git main You can add an alias to your bash or other shell to help:\nalias git_init=\u0026#39;git main\u0026#39; Alternatively use this alias:\nalias git_init=\u0026#39;git init \u0026amp;\u0026amp; git checkout -b main\u0026#39; Don\u0026rsquo;t forget to source your changes to your shell. For bash you can run: source ~/.bash_profile.\nWhich gives you the ability to run either git main or git_init to start a new repo with main as the default branch.\nGithub # The branch protection rules in Github will need to be modified for each repo that you touch.\nCircleCI Changes # In the UI ensure that the pipeline is filtering for All Branches instead of just master. The new builds should show up automatically. Historical information for old builds is not destroyed in CircleCI during this change. So no changes should be required.\nWhen things don\u0026rsquo;t work there are steps to re-sync the project that should be used as a last resort:\nVCP Default Branch Isn\u0026rsquo;t Reflected on CircleCI CircleCI Doesn\u0026rsquo;t Notice when github default branch is updated\nTerraform Registry # Terraform registry uses git tags to pull from github. As long as main is a reflection of master then no changes are needed for the terraform registry.\nIDE Changes # Any ID that handles git integrations should be able to handle this change. Consider adding main to the set of protected branches if your editor allows for that.\nTooling Changes # As a quick reference for some tools that need to be updated here is a non-exhaustive list:\nDangerJS should be updated to at least v10.3.0 (see the PR) git should be updated to at least v2.28.0 (see the Release Notes) References # References:\nGit Renaming the master branch "},{"id":24,"href":"/docs/develop/source-control-repos/","title":"Source Control Repos","section":"Docs","content":" Git Repos # This page provides guidance on how to set up and manage your Git repos. For Rhinos, most of these will be kept in GitHub, so much of this advice will be geared specifically to managing GitHub repositories.\nSetting Up Repositories # We try to manage our Git repositories with Terraform as much as possible. However, we haven\u0026rsquo;t had great luck creating repos with Terraform, so what we do is create the repo manually in GitHub, then import it into Terraform. For an example of how we\u0026rsquo;ve done that, and what Terraform code for GitHub repositories looks like, you can look in the legendary-waddle repo.\nRepo Maintenance and Hygiene # Avoid including files in source control that are specific to your development machine or process. For example, your editor or OS might generate files that are not pertinent to your project. Since those types of files will need to be ignored from all the repos you work on, it\u0026rsquo;s more convenient to set them once in a global .gitignore file (typically named ~/.gitignore_global), as opposed to in each repo\u0026rsquo;s .gitignore. Once you create your file and add rules to it, you can tell Git to use it: git config --global core.excludesfile ~/.gitignore_global Delete local and remote feature branches after merging. This allows us to reuse branch names, and makes it easier for you to differentiate between your branches that are done and those that are still being worked on. To turn this on in GitHub, check the \u0026ldquo;Automatically delete head branches\u0026rdquo; box under the \u0026ldquo;Merge button\u0026rdquo; heading in the Options settings for your repo. If you\u0026rsquo;re maintaining the repo with our terraform-github-repo module, it will turn this on automatically. GitHub Repo Settings # Protect the default branch by turning on the following settings at a minimum: Require pull request reviews before merging Require status checks to pass before merging Include administrators If you\u0026rsquo;re maintaining the repo with our terraform-github-repo module, it will turn this on automatically.\nSplitting Out Code to a New Repository # If you need to split out code from one of your repositories into its own separate repo, you can follow these steps to make sure you\u0026rsquo;re preserving the commit history when doing so.\nThe filter-branch subcommand to git can be used to accomplish this, but the documentation strongly discourages its use. Instead, a third-party utility called git-filter-repo is recommended.\nFirst, create a new repo using the GitHub UI as you would normally.\nIn your terminal, clone a copy of the original repo into a new folder:\ngit clone git@github.com:OWNER/REPONAME.git NEWREPO \u0026amp;\u0026amp; cd NEWREPO Filter out commits that change the specified directory:\ngit-filter-repo --subdirectory-filter DIRNAME The origin remote will be removed in this process. Add it back with the new repo URL, and push the filtered work tree:\ngit remote add origin git@github.com:OWNER/NEWREPO.git git push --set-upstream origin \u0026lt;main\u0026gt; Note: This repo should be public and properly licensed. SHOC has a 🔒 decision record on what licenses should be applied to which sorts of projects.\nIf your project is managing its GitHub repositories with Terraform as we suggest, make sure you add the repo to the Terraform code and import it from GitHub. Speak to your project\u0026rsquo;s infrasec team if you need help with this.\n"},{"id":25,"href":"/docs/develop/source-control-tools/","title":"Source Control Tools","section":"Docs","content":" Tools # This page provides a summary of tools we commonly use for source control.\nGit # Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.\nUse your work email when making commits to our repositories. The simplest path to correctness is setting global config:\ngit config --global user.email \u0026#34;username@domain\u0026#34; git config --global user.name \u0026#34;Full Name\u0026#34; If you drop the --global flag, these settings will only apply to the current repo. If you ever re-clone that repo or clone another repo, you will need to remember to set the local config again. You won\u0026rsquo;t. Use the global config. :-)\nFor web-based Git operations, GitHub will use your primary email unless you choose \u0026ldquo;Keep my email address private\u0026rdquo;. If you don\u0026rsquo;t want to set your work address as primary, please turn on the privacy setting.\nNote that with 2-factor-authentication enabled, in order to push local code to GitHub through HTTPS, you need to create a personal access token and use that as your password.\npre-commit # We use pre-commit to easily add git hooks to our Git repos. This allows us to automate things like checking for merge conflicts or mistakenly adding secret keys in our code. See this example pre-commit config file from one of our projects.\nSince git does not distribute hooks when a repository is cloned, you will have to install pre-commit in each cloned repo manually using pre-commit install --install-hooks or pre-commit will not run in that repo. To assist with automating this step, pre-commit has a feature to exploit the template directory setting in git:\ngit config --global init.templateDir ~/.git-template pre-commit init-templatedir ~/.git-template From now on, each new repository you create or clone will have pre-commit installed automatically.\npre-reqs # We use pre-reqs to bootstrap system pre-requisites that are required to run the code we push to Github.\n"},{"id":26,"href":"/docs/develop/source-control-workflows/","title":"Source Control Workflows","section":"Docs","content":" Git Workflow # We\u0026rsquo;ve tried to come up with some standardized workflows for working with Git. This page hopes to capture them so that new Rhino\u0026rsquo;s can work cleanly with other developers and minimize friction.\nBranching Strategy # There are several well-established branching strategies and workflows out there. This article covers four leading types.\nEach project will have its own requirements, and sometimes even individual projects might need different approaches, but generally SHOC recommends the \u0026ldquo;GitHub Flow\u0026rdquo;, for its lightweight process and ease of use with CI/CD.\nHowever, the \u0026ldquo;Git Flow\u0026rdquo; process is useful in scenarios where you have a slower release process, e.g. for mobile apps that have a App Store as part of its critical release path.\nFork vs Clone # If it\u0026rsquo;s a repo you have write and push access to, clone it. Otherwise, fork it.\nCloning with SSH vs HTTPS # Use HTTPS for read-only repos (where you’re only ever going to clone and pull), and use SSH for everything else. SSH supports certificates on security keys, and HTTPS does not. Therefore, we should not use HTTPS for anything that requires authentication. However, if you’re just pulling a public repo, HTTPS is more convenient since you don’t need to authenticate anything (unlike SSH for read-only repos).\nWorking locally # Pick either rebase or merge to incorporate changes from the default branch onto your local branch. Use the same method every time for consistency.\nPR Size # Pull requests should be small and focused enough to be reviewable in under 15, or ideally under 5, minutes. Small PRs have a better chance of being quickly accepted. The longer the PR, the more an engineer has to keep in their head, and the more contiguous time they need to schedule for the review. This might lead to a less thorough and delayed review, which could block you from your other work. Long-lived PRs also often require more frequent updates with the main branch, which could result in mistakes when conflicts are present.\nAfter creating a PR in GitHub # Once a branch has been pushed to GitHub and a PR has been submitted for review, any changes should be made via individual new commits. Never force push while a PR is under review. This avoids issues such as merge conflicts or accidental overwriting of code when others are using your branch.\nCommit messages # For each meaningful commit, write a good commit message following these seven rules:\nSeparate subject from body with a blank line Limit the subject line to 50 characters Capitalize the subject line Do not end the subject line with a period Use the imperative mood in the subject line Wrap the body at 72 characters Use the body to explain what and why vs. how Example:\nReplace chromedriver-helper with webdrivers `chromedriver-helper` says in bold on their README that they are no longer maintaining it and that people should use `webdrivers` instead. Additionally, `webdrivers` has a very useful feature that will automatically use the right version of `chromedriver` based on the version of Chrome that is installed on the machine. This was not possible with `chromedriver-helper`, which made it harder to deal with versioning locally and in Circle CI. Resolves #123 "},{"id":27,"href":"/docs/develop/source-control/","title":"Source Control","section":"Docs","content":" Source Control # Overview # Source control has less to do with the VCS system you use, and more to do with the stages code goes through on the journey from inside a developer\u0026rsquo;s head, tested and merged into the main line, deployed to production, and thence in time to being identified as that accursed legacy system that is causing all the problems.\nMost of our projects will use Git as their VCS, and usually with GitHub as the host for their repositories, but this is not a hard and fast rule (clients will often host their own repositories in their own system).\nContents # Tools Git Repos Git Workflow Master to Main branch rename "},{"id":28,"href":"/docs/develop/standard-delivery-pipeline/","title":"Standard Delivery Pipeline","section":"Docs","content":" Standard Delivery Pipeline # The following is a standardized workflow for developing and deploying our code using CI/CD. This is meant to be an example, not a prescriptive ideal; if your project needs to use a different workflow, that\u0026rsquo;s fine. Your goal should still be to provide a workflow that can support frequent updates, good testing, and prompt deploys.\nDeveloping Your Code # We generally use Git for our code repository, and Github more specifically. Our preferred method for making changes and integrating them into the project is relatively simple:\nCheck out the repository for my-project. Create a new branch my-feature in the repository. Make your changes to the code and commit them, then push your code to the central repository. Create a pull request, get approvals, and then merge your my-feature branch into the default branch. The virtue of this workflow over more complicated methods like the one shown here is that we are constantly merging to the default branch, so the drift of any specific branch from the default branch is kept to a minimum. The idea of more frequent small changes over less frequent large changes is a fundamental aspect of many development practices (such as Agile). The core of this belief is that things which break the the application can quickly be noticed, isolated, and rolled back (or fixed), without requiring a lengthy period of diagnosis, or requiring delicate operations to pull out problem code without sacrificing code which is benign.\nThis process is known as continuous integration or CI (because code is constantly being integrated into the mainline of the project). Its relative simplicity belies the fact that it also requires a great deal of additional work to ensure that this process can occur without disrupting the project\u0026rsquo;s development.\nUnit Testing # Key to ensuring that our CI workflow is safe is making sure we are doing unit testing of our code prior to merging in to the default branch. To do this, we use automated tools like CircleCI to run a battery of tests against every branch we create a PR from. We want to run a variety of tests that cover a variety of things like:\nCode formatting and syntax Acceptance testing for configuration Functional code testing Our goal is to be as sure as possible before anyone even looks at the pull request that this code does what we want and won\u0026rsquo;t break things.\nDeploying Our Code # Merging your code is just the first step to actually getting it in front of a real person to use it. The other component for this how your code gets from a Git repository into a live environment where someone can touch it. This workflow is orchestrated via an automated tool like CircleCI, and looks something like this:\nNew code is merged into the default branch. Jenkins detects that the default branch has been updated and deploys the code to our development environment. Jenkins checks to make sure the deploy was successful. Is our environment running the right version of the code? Jenkins performs post-deployment testing. Does it pass some functional tests to ensure user workflows are functional (eg, can someone log in, pull up a user record, put things in a shopping cart, etc)? Are logs filling up with error now that the new version is deployed? If the tests pass, Jenkins goes ahead and deploys to our deployment environment, then runs the same sort of tests it ran in development. In some cases, there may be an additional manual approval step, or this may be a deployment to a staging environment as an additional step prior to \u0026ldquo;real\u0026rdquo; deployment. This means that we are constantly releasing new versions of our code to users \u0026ndash; we may deploy a dozen times a day, all with a reasonable degree of confidence that everything will be fine, thanks to our automated tests. If something doesn\u0026rsquo;t work, we can stop the process (or roll back, if the problem is detected post-deploy), and users will be protected from malfunctioning code. This requires that we write a battery of tests at every stage of the release process, however.\nThis automated workflow that allows rapid release of merged code is called continuous delivery or CD.\n"},{"id":29,"href":"/docs/develop/tools-and-practice/","title":"Tools and Practice","section":"Docs","content":" Tools and Practice # Overview # This section addresses the tools and practices which are part of the everyday habits of being a Software Engineer at the 805th. If you\u0026rsquo;d like to get a sense for how Rhino\u0026rsquo;s solve problems, take a look at the 🔒 interview solutions repo. If you haven\u0026rsquo;t already, post your solution!\nSHOC aims to find a balance between giving the autonomy and tools to make the best choices in any given situation, but to also avoid revisiting the same questions again and again. The latter is especially true when the question is not material or core to how we do business. In some cases, we have strong opinions about which questions are not valuable as a cause for debate, e.g. Tabs vs Spaces.\nGenerally, our approach should be \u0026ldquo;if there is a suggestion or answer in these pages, follow it until you have a compelling reason not to\u0026rdquo;. Once you have a compelling reason to change the practice document it by adding to or updating these pages.\nNote on Default Branches # Where possible the documents here should refer to the Default Branch instead of using the word master. Where this is not possible try to use the default branch name main. In links to repos in GitHub please also attempt to get the reference to main and not to master.\n"},{"id":30,"href":"/docs/documentation/external-resources/","title":"External Resources","section":"Docs","content":" External Resources # This is a list of articles and talks that have influenced the advice in this section; you may find it useful to read or view these if you\u0026rsquo;re especially interested in this topic.\nArticles # 10 Tips For Making Your Documentation Crystal Clear, Ben Cotton A Primer on Documentation Content Strategy, Stephanie Blotner What to Write and Technical Style, Jacob Kaplan-Moss Yelp Production Engineering Documentation Style Guide, Chastity Blackwell Content debt: What it is, where to find it, and how to prevent it in the first place, Melody Kramer Keep Refining, 18F Talks # The 7 Deadly Sins of Documentation, Chastity Blackwell Scalable Meatfrastructure, Alice Goldfuss Traps and Cookies: A Mystery Package From Your Former Self, Tanya Reilly "},{"id":31,"href":"/docs/project_management/accessibility-standards/","title":"Accessibility Standards","section":"Docs","content":" What is accessibility? # Accessibility (also abbreviated as a11y) is making sure that the greatest number of users are able to access and use your product.\nAccessibility is inclusion # Accessibility is not just making sure that a web application is usable by those with physical limitations. While it is very important for those that rely on screen readers and other assistive devices to be able to use the application, other limiting factors can include: type of device, language, content, bandwidth, etc. Ideally we wish to include as many different types of users and situations to be able to access our application as realistically as we can.\nAccessibility compliance and beyond # All software projects need to be ADA compliant in some way. That makes it a legal requirement to ensure all digital products are accessible. Common thresholds for compliance include 508 and WCAG.\nYou can be compliant and still not make accessible software. True accessibility is about making sure things work for people.\nAccessibility is important during the entire project lifecycle # Accessibility needs to be incorporated throughout the entire lifecycle of a project, it should not be incorporated as an afterthought or checked only towards the end. It is much easier to address issues during feature conception, development, and story creation phases; as a consequence, it will save time and therefore become more cost effective for the project.\nAccessibility is important across practices # Every role in the project should be aware of the importance of accessibility and have a basic understanding of how they are addressed (even if they only know to look at this guide as a reference first). Accessibility should not be beholden to only one practice to address (e.g. just engineers to fix tags or designers to figure out color and content), but it should be the shared responsibility of everyone on the project. An effective team can work together to make sure that accessibility concerns are met throughout the entire project lifecycle.\nAccessibility also applies to the processes used by a team during a project. Many disabilities are invisible and/or undisclosed, so it’s best practice to uphold basic accessibility standards at all times. Some good practices for holding accessible meetings, writing accessible documentation, and using accessible tools include:\nMaking sure that documents have content structure e.g. headings so that assistive technology users can effectively navigate the content Share meeting materials in advance of scheduled meetings Provide captions and transcriptions for meetings Using the tool’s built in features e.g. Zoom hand raise Additional Resources for Accessibility # Accessibility Wiki - The wiki covers a range of topics including various form elements, patterns, implementation suggestions with rationale, examples and specific guidance on how to design and build various components Drafting an Accessibility Plan - A guide to developing an a11y plan for your project and executing on it Sample Github PR template - PR template that teams can use to include accessibility requirements into their pull request process. Includes specific checklists for Q\u0026amp;A. Sample breakdown of accessibility testing process (linked in PR template) Sample diagram of a cross-practice development acceptance flow "},{"id":32,"href":"/docs/project_management/agile-project-beta-phase/","title":"Agile Project Beta Phase","section":"Docs","content":" How the beta phase works # The beta phase is where you take your best idea from prototyping and start building it for real. It also involves thinking about how your service will integrate with (or start to replace) existing services, and preparing for the transition to live.\nStructure your beta phase so you can roll out the service to real users - while minimizing risk and maximizing the potential to learn and iterate the service. Make sure:\nsupport staff are aware and engaged on changes to system and user flows 805th and clients are prepared to adapt to how new users may use the system in unforeseen ways You’ll start out in ‘private beta’. This involves inviting a limited number of people to use your service so you can get feedback and improve it.\nOnce you’ve improved the service and are confident you can run it at scale, you can move into ‘public beta’. This involves opening up your service to anyone who needs it. If you’re replacing a legacy service, keep the legacy service running until your new service moves into its live phase.\nThings to pay attention to in beta # By now you’ll have developed a strong understanding of your users’ needs and, by the end of prototyping, chosen a way of meeting those needs.\nDuring beta, focus on making sure that the solution you’ve chosen works as well as possible by carrying out user research and starting to gather data on how successful the service is based on the success metrics you identified in prototyping. Iterate the service based on what you learn.\nSolving a whole problem for users # Getting the scope of your part of the journey right # At prototyping, you’ll have tried different approaches and developed an idea of how to scope your transaction so it makes sense to users.\nUse the beta phase to continue to test and refine that scope.\nJoining up with the user’s wider journey # At prototyping, you’ll have worked out whether your service is part of a wider journey. At the end of your beta phase, you’ll need to show how the service you’ve built operates within that wider journey, working across organizational boundaries where necessary.\nWorking in the open # Continue to work in the open during beta - for example, by blogging and by inviting operational delivery colleagues like the back office team, customer support team etc to open show and tells, demos so they know what you’re doing.\nIf what you’re building at beta is going to be part of a wider journey involving other organizations or services, it’s especially important to talk publicly about your plans. It’s also worth looking into whether you could start or join a service community.\nDealing with constraints # You’ll have used the prototyping phase to understand any constraints that are likely to affect your service.\nFor example, in prototyping you identified the constraint that contracts or the organization’s plans for a wider change program might influence how fast you can move away from legacy technology. In beta, you can create a rollout plan to slowly shift from the legacy platform.\nAccessing user’s information securely # If you’re building a service that reuses information users have already provided, you’ll need to show that users’ information is being shared in a way that’s secure, stable and works at scale. This might be through an Application Programming Interface (API) that follows the API standards of the client.\nYou’ll also need to make sure any information sharing happens in a way that protects users’ privacy.\nProviding a joined up experience across different channels # You’ll need to show that you’re making reasonable progress in improving the user’s experience in different channels. “For example, this could be ensuring that the offline part of the Milmove journey is positive. Or signposting about your new service in other places where users currently might go for help.\nYou should be able to explain how you know your assisted digital support model meets the needs of users who need help doing things online. And how you’ve set up your user support model.\nYou should be able to explain how you’re involving colleagues from operational delivery (back office team, customer support members etc) in:\nprioritizing what you work on designing how the service works - for example, by inviting them to attend and analyze user research, participate in brainstorming sessions. You should also think about how your product impacts wider existing process and bubble it up to the client if changes needs to be made elsewhere.\nMaking sure everyone can use your service # As part of providing a service that everyone can use, you’ll need to show that your service is accessible. This can be from running accessibility tests and running research sessions with disabled people.\nYou’ll also need to talk about the results of your accessibility audit and fix any issues before moving into public beta.\nYou’ll need to show that you’ve considered whether the service has any pain points that might lead to people being excluded, and what steps you are taking to address them.\nOther things to consider at beta # You’ll usually need to talk about how you use technology, for example:\nthe way you deploy software, proving you can deploy frequently without impacting users how you’ve made your service secure so users’ data is safe how you’ll work with cookies and similar technologies how you’re making source code open how you’re managing the limits placed on your service by the technology stack and development toolchain you’ve chosen how you’re using open standards and common platforms what the effect would be if your service was unavailable for any length of time and how you’re managing this how you’re testing your technology Moving from public beta into live # You’ll have your go-live assessment at the end of the public beta phase. Spend public beta preparing for live.\nRunning your service during live # You’ll need to work out how to run your service sustainably during live. This does not necessarily mean having an agile team on the service 100% of the time. Spend time during public beta working out what level of continuous improvement it makes sense to support, and who you’ll need on the team. You will want to include factors such as:\nbug triage load refactors and technical debt strategies package and security updates new compliance requirements As in beta, improvements you make during live should be:\nbased on user research tested to make sure they work with different browsers and devices tested for accessibility quality assured You should also make sure you are clear on the effects that changes will have on offline channels (such as: in office organizational memos and paperwork), or any related services - and make sure none of your changes will have a negative impact on user experience.\nYou’ll also need to spend time during public beta reviewing the performance metrics you set to check the data you’re collecting will tell you whether your service is performing as it should.\n"},{"id":33,"href":"/docs/project_management/agile-project-go-live/","title":"Agile Project Go Live","section":"Docs","content":" How to Go Live # Things to pay attention to when going live # Before the service moves into the live phase, you’ll need to show that you’ve used the beta phase to build a service that meets the needs you identified at discovery and prototyping, testing, and iterating based on what you learn.\nThis should be coordinated with the client organization to decide what is required to release the software to all users. While they should be coordinated early and often, directly before going live, the following may occur:\nsecurity assessments (e.g. penetration tests, securing your Authority to Operate) accessibility audits performance testing Other things to consider before you go live # Before the service goes live, you’ll also need to make sure:\nyou’re securing the service’s information you’ve got appropriate metrics in place to measure the success of the service, based on what you’ve learned during beta the service meets accessibility requirements you’re able to maintain uptime and availability and monitor the status of the service you’re able to continue with vulnerability and penetration testing you’re able to continue testing service performance you’re able to maintain quality testing and code coverage you have addressed, or have plans to address, any pain points that might lead people who use your service to be excluded Iterating in live # The live phase is about supporting the service in a sustainable way, and continuing to iterate and make improvements. It is about continuing to iterate towards solving a whole problem for users.\nThis means your team will continue to:\nwork with teams responsible for related services address any constraints you identified in beta (for example with legacy technology learn from real usage including metrics and user research make sure that transactions are scoped in a way that makes sense to users identify common software patterns that can be open-sourced transition or integrate any existing transactions that meet a similar need to yours "},{"id":34,"href":"/docs/project_management/agile-project-hand-off/","title":"Agile Project Hand Off","section":"Docs","content":" Handing Off Your Service # Most commonly, client engagements end when a contract finishes with no renewal. When a this happens, it’s important to follow a set of guidelines to properly support users, set up new service teams for success, and close the client relationship amicably. This section contains some guidelines for how you can make that happen successfully.\nSupporting the users # Making sure the users are supported through transition is the top priority when handing off.\nCheck that revoking any individual accounts when offboarding won’t disrupt service. This includes any third party subscription renewals, such as DNS records for the application.\nCheck that any in flight communications with users, such as triage tickets, are accounted for.\nDuring this time, focus less on new feature work (if at all), as it could leave the users with incomplete or poorly supported experiences. This is a better time to work on any outstanding bugs or refinements.\nHanding off your work # At a minimum, any artifacts from the project should be handed off to the client. This includes, research findings, designs, application code, and infrastructure.\nIdeally, access will have been properly shared throughout the project to make this as seamless as possible. However, different members have different access needs (for example, a product owner may not have AWS access), so make sure to work with the client to hand off that access to whom it is applicable.\nIf a new team is taking over then you should work to handoff that work. Commonly, there will be an overlap period where both teams will be working on handoff. Share your experiences, findings, and knowledge of the user with them so that the new service team can learn from your work.\nIf an application is running, help the new team get set up with the developer environment and review the application architecture. Ensure they have the ability and understanding to deploy new code.\nClosing out with client stakeholders # Before offboarding, check in with the client about what their needs are for a successful handoff.\nSome helpful activities during this time are:\nHave a retro. Focus on end-to-end project lessons. Help build a future roadmap for the client based on the work the team has done, their priorities, and capacity. Celebrate your accomplishments. Look back at the goals of the project and the work you’ve done together. "},{"id":35,"href":"/docs/project_management/agile-project-prototyping-phase/","title":"Agile Project Prototyping Phase","section":"Docs","content":" How the Prototyping Phase Works # Prototyping is where you try out different solutions to the problems you learned about during discovery. Prototyping is done using the steel cable work as a foundation.\nSpend this phase building prototypes and testing different ideas. And do not be afraid to challenge the way things are done at the moment: this is a chance to explore new approaches.\nYou do not have to prototype the user’s entire wider journey.\nYou might not even want to prototype all of the transaction or element you’re working on: often it makes sense just to focus on the areas you think will be most challenging. This lets you do the minimum you need to test your riskiest assumptions.\nWith any idea or prototype you try out, build things that are just complex enough to let you test different ideas, not production quality code. Expect to throw away any code - and lots of the ideas you test - at the end of alpha.\nBy the end of prototyping, you should be in a position to decide which of the ideas you’ve tested are worth taking forward to beta.\nThe prototyping phase tend to last between 6 and 8 weeks. At the end of this, you should do an assessment to determine if the effort was successful enough to move on to the beta phase.\nIt’s okay to decide at the end of your discovery that you do not think it’s worth moving head into prototyping.\nFocus on testing your riskiest assumptions # A crucial part of prototyping is identifying your riskiest assumptions and testing them. What these are will depend on the service you’re building. For instance, on SABER, we needed to test our methods of talking to third parties for identity verification, because we wanted to use step functions, an AWS product we hadn’t used before. We prototyped this by rigging up a step function to ping a simple API (in this case, a weather API) just to make sure we understood how this worked.\nIf you are constrained by legislation, you will need to find out if there are feasible ways of accomplishing your goals within the current legal framework, or if you will have to reduce the functionality to work within the limits of legislation. You could also articulate what would be possible if there were legislative changes.\nThings to pay attention to at alpha # There are a couple of points in the Service Standard you’ll want to pay particularly close attention to at this phase.\nSolving a whole problem for users # One of our goals for any feature is to solve a whole problem for a user.\nIn prototyping, this could mean showing:\nhow you know that you’ve got the scope of your part of the journey right you’ve looked at the wider user journey your service is part of you have a sense of what would need to happen to make the journey as a whole work as well as possible (in particular, you’re able to talk about other services that are part of the same journey, and the opportunities and challenges involved in making changes to those services) you’re working in the open, and are collaborating with stakeholders and other contractors working on related projects that are part of the journey where it makes sense to do so you understand any constraints with legislation, contracts or technology that impact on your service you’re planning to minimize the number of times a user has to submit the same information into the system Getting the scope of your part of the journey right # Getting the scope of a transaction right is probably the most important part of making it simple to use. Use prototyping to explore what scope makes sense from the user’s point of view.\nJoining up with the user’s wider journey # Not all transactions are part of a wider journey for the user. But if yours is, you should have explored that wider journey as part of discovery.\nYou could bring a rough journey map (or similar artifact) to your prototyping assessment, showing where your service sits in the user’s journey, the different organizations involved and the different channels people use to access them.\nIf one of your riskiest assumptions is that it’s possible to make changes to other services in order to provide the user with a simple, intuitive journey, use the prototyping phase to test that assumption by talking to the people responsible for those other services. By the end of prototyping, make sure you’re clear what can be changed and how difficult or costly it would be to make those changes.\nIf you face any blockers collaborating within the client organization or elsewhere, you need to show how you’re addressing them, for example by building relationships with potential collaborators.\nEither way, you should be talking and building relationships with your client and other stakeholders, as well as other people working in the same space, such as the Digital Service Coalition.\nDealing with constraints # Use the prototyping phase to explore any immovable constraints in legislation, contracts or legacy technology that affect the service you’re planning to build. By the end of prototyping, you should be able to explain:\nhow you’ll create a service that meets user needs while working within these constraints. where they’re the type of constraints that can be removed over the long term and the organization’s plan for doing this. For example, by changing a technology platform or contracting with suppliers in a different way. Working in the open # During prototyping, you should continue to talk with your team, your project, and your practice about the prototypes you’re building and what you’re learning from them. This could be done in a weekly demo, a shared document, or even a Slack thread. Consider if it’s worthwhile to distribute this wider in a more formal environment, like an OTT.\nReusing users’ information where possible # It’s often the case that, when interacting with related services, user data might already be available in some format within the service.\nUse the prototyping phase to investigate whether you can reuse that data, so that users do not have to provide the same information multiple times as they move from task to task. Unless there are public policy, trust or legal reasons not to share data, you’ll be expected to show how you’re working towards sharing it.\nProviding a joined up experience across different channels # Your feature or service will need to work well across all the channels a user might use to access it.\nThis involves understanding how the online and offline parts of your service link together and any pain points users experience as a result of this.\nYou could work towards this in prototyping by:\nincluding offline elements like letters in your prototyping experiments and user research, especially where this relates to a risky assumption (for example, that you’ll be able to change the content of letters) considering user journeys that start within a third party organization, like referrals inviting feedback from other members of your practice during the prototyping – they’ll have a really useful perspective on what the riskiest assumptions are Making sure everyone can use your service # With an online prototype, you cannot apply the full range of technical accessibility tests you’d use for production code. But you should be able to show that you:\nunderstand the WCAG accessibility principles – this will help you identify and test any specific accessibility challenges you’re likely to face with your service are including disabled people in your user research By the end of prototyping you should have a plan for how you’re going to tackle accessibility at beta.\nYou should also be able to show that you’ve considered inclusion in the wider sense, where applicable to your target audience. For example, this may mean:\nyou’ve thought about whether there are likely to be pain points for particular groups of people when accessing the service (for example, if you’re asking for someone’s permanent address and your users include homeless people, you’ll need to show that you’ve got a plan to stop them being excluded) including people with low levels of digital confidence in your user research Deciding whether to move on to beta # Prototyping is finished when you’ve got a prototype that’s substantial enough to help you make a decision about whether to move on to the beta phase or not.\nTo move on to beta you’ll need to be confident that:\nyou can create something that meets users’ needs and is cost-effective you’ll have the budget and people necessary to deliver what you need to You should be able to explain how you came to this decision using the success metrics you identified at the end of discovery.\nIf you get to the end of your prototyping and you’re not confident you could do these things, you could stop altogether or decide to repeat discovery or continue prototyping.\nOther things to consider at prototyping # When you’re confident you want to move on to beta, your project team and client should start working on a product brief. This should include an assessment of whether your team has the right people for success.\nThere are a few other things that you’ll need to consider in this phase. You’ll need to show that you’ve started to think about:\nthe sorts of programming tools you’d like to choose for beta and why you’d get value for money how you’ll identify threats to your service, how you’ll deal with them and how you’ll stay up to date with threats whether or not you’ll open source your code; SHOC has a preference for doing this where possible, but it may depend on the client whether or not you’re going to be using common platforms how your users would be affected if your service had technical problems You should also continue to refine the metrics you’ll use to measure how successful your service is.\nIf you’ve created any new design patterns - or learned anything useful about an existing design pattern - you should share what you’ve learned in the SHOC playbooks.\nRelated guides You may find these guides useful:\nHow the discovery phase works How the beta phase works How the live phase works Retiring your service Source Credit: This content is an edited reprint from the GOV.UK Service Manual under the terms of the Open Government Licence v3.0.\n"},{"id":36,"href":"/docs/project_management/definition-of-done/","title":"Definition of Done","section":"Docs","content":" Definition of Done # The definition of done (DoD) is when all conditions, or acceptance criteria, that a software product must satisfy are met and ready to be accepted by a user, customer, team, or consuming system.\nProduct Plan A definition of done can be written for a user story, task, or bug.\nWhy it’s important # The definition of done is a tool we use to ensure consistent, high-quality work; and help the team decide when we have finished a piece of work and can move on to the next thing. By defining the work in precise terms, we ensure we’re spending our time and money on delivering the highest value. It helps us avoid churn and keep the train moving. Every SHOC project team should have an agreed upon definition of done that’s specific to the project.\nWho uses it # Engineering uses it to ensure a certain level of code integrity or all tickets. Product managers use it to ensure work meets user and/or business requirements. They also use it to align with the other practices on what gets built. Finally, the client engagement and delivery managers can use it to ensure we’re delivering against contractual obligations. How we do it # We try to include the following items in any definition of done, whether it be for a user story-, task-, or bug-type ticket. Use these as a starting point, then add detail as needed for your project.\nThere’s alignment with success metrics. Work matches the designs. User acceptance tests pass. Tests are written to project specifications. Relevant documentation is complete. Code has been peer-reviewed. Code is released in production. In drafting your definition of done, here are some helpful questions to consider:\nWhen we are done with the work, what absolutely must be true? Which of these things are like each other? What criteria can we consolidate? Tips, things to consider # There might be definitions of done at the user story/task/bug level, and also at the project phase level. The definition of done can be written as acceptance criteria in a user story. It can be helpful to surface trade-offs when writing the definition of done. Learn more # Product Plan: The Agile Definition of Done\nMountain Goat Software: Why Definition of Done Matters\nAppendix # Terms and definitions # Done - we can move on to the next thing.\nTests - manual or automated methods used to validate that the thing produced will fulfill the requirements of the user/stakeholder\nAcceptance - the user/stakeholder/etc’s defined needs/wants are fulfilled.\nDocumentation - any written material that is used by users or developers to understand the work item produced and how it fits into the larger picture.\n"},{"id":37,"href":"/docs/project_management/the-steel-cable/","title":"The Steel Cable","section":"Docs","content":" The Steel Cable # a.k.a. spanning cable, guide wire, tracer bullet\nImagine you are standing on one side of a canyon looking over at the other. We need to eventually move a lot of things across that gap, and we’re tempted to start thinking about the grand bridge that will move passenger cars, freight trucks… or maybe even a train! But first, we want to figure out how to string one solitary steel cable across the canyon. That steel cable will help us answer a lot of important questions. How long is it? Where do we anchor these first attachments? Is this the right spot in the canyon?\nSometimes the cable becomes a key part of the eventual bridge, sometimes we keep just one anchor, and sometimes we end up moving the whole thing. It’s better to build the cable and adjust it than wait until you know the perfect location. Furthermore, we ensure that we can always get something across. Once it’s up, we’re never totally cut off from our end goal.\nSo, What is a Steel Cable? # The steel cable is generally what the team builds first. It allows the engineering team to start doing valuable work while the details of user value are still being determined. It reduces the risk in the basic project infrastructure. It is a foundational component for the later work of building the minimal user journey or MVP.\nA more general way to describe this is: If you draw the architecture diagram, you should have something real that works in every block, even if it doesn’t do anything useful other than accepting a write and returning a read.\nWhile the steel cable is centered around an engineering outcome, other practices (ie. design, product) can get a better sense of what their interest in the steel cable might be from the sections below.\nWhat Isn’t a Steel Cable? # Features. The whole point of the cable is that you don’t need to know exactly what the user experience is or what they need yet, but you have the basic idea of architecture, so you should set that up first.\nOverall, the steel cable analogy doesn’t imply that you need to have everything in place up front; if you aren’t sure you need the cache, you shouldn’t add it. It is expected that the team will have some discussion about what is and is not included.\nExample of a Steel Cable # The steel cable is an exercise in determining technical (ie. infrastructure, tooling) scope and reducing risk. In the GROWS “tracer bullet” definition, it says:\n“It doesn’t have to do anything more than a “Hello, World” level program, but it needs to have all the pieces working together, from front end to back end.”\nThe steel cable means that you have data flow set up between all components of the application at an infrastructure level (where infrastructure explicitly includes front end frameworks).\nSo, for example, you’re building a web application. Based on your initial understanding of the problem space, you believe you will need one load balancer talking to a tier of application servers, talking to a Postgres database AND data in an S3 bucket AND a caching system.\nIn this case, SHOC defines the steel cable as the minimal infrastructure for:\na load balancer that is configured to talk to… your application servers, which can receive a HTTP request S3, which app servers can connect to Postgres database, with managed migrations, which app servers can connect to your caching system (redis, etc) the delivery pipeline (CI) needed to build and deploy the above automated deployments to dev and prod If your application is supposed to write data into those systems, the steel cable means that your application writes 1 record into each system, and reads it back for display to confirm that it works.\nNote, this is just one example of a steel cable (in this case, a web app). If you know your application will need a frontend framework, it may involve rending “Hello World” on a React page with a build pipeline for the assets. Or, if you have a mobile app as part of your system, then you’ll want to see the same in an iOS build. Further, if 3rd party integrations are crucial to your application, the steel cable includes creating those connections. What infrastructure is included in your application will be specific to your project and needs.\nWhat’s the Value? # The primary value of the steel cable is a massive reduction in project risk. It forces you to confront access control, deployment, automation, CI, test infrastructure at the beginning of the project. All of those feel “easy” but in practice are fraught with peril. Plus, once you have something working, it’s much easier to edit it. Your application engineering flow is much smoother if the workflow is\n“code, test, confirm, commit, watch the robot deploy and validate, repeat” than\n“code, test, realize you need a DB, swear, try to figure out how to add that in your feature PR, swear some more, shave the yak, look up, it’s 2 weeks later” It’s a form of the “go slow to go fast” heuristic.\nWhen Do You Build One? # Assuming you have access to the environment, your goal is to complete this by the end of the first sprint. If you think you can’t get it done in the first sprint, then consider whether you’ve over-scoped the cable. Alternatively, it could be a sign of a problem:\nthe team lacks training on the selected tools lack of access to required systems improper team sizing These issues should be identified and discussed within the team immediately.\nThis proposed time frame is a guard rail so that teams aren’t spending months building something they think is the steel cable. Realistically, there might be circumstances in which you need more time–but the steel cable should be as simple as possible.\nIn terms of project timeline, this can often (not always) be parallelized with early discovery phases.\nHow Do We Use It? # Once you have the steel cable established, all future changes are iterative: you’re always migrating from a working state to a new working state. A working steel cable web app could be verified by going to https://my-system-steel-cable.foo.bar and having that return an HTML page that looked like:\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;Hello, world\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;I wrote \u0026#34;It worked!\u0026#34; into the database.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;I read this value from the database: \u0026#34;It worked!\u0026#34;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;I wrote \u0026#34;It worked again!\u0026#34; into S3 at /my-bucket/foo.txt\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;I read this value from /my-bucket/foo.txt: \u0026#34;It worked again!\u0026#34;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; …the minimal amount to show that it worked: no styling, no extraneous javascript libraries, and renders a simple string component. If you break something, you fix it immediately. No breaks once the cable is built, all future iterations of the application are built from this.\nIn Closing # The goal of the steel cable is simplicity – you are just trying to make sure the F/E, B/E, and infrastructure (including deployments) can connect. Nothing more. The value of the steel cable is primarily for the internal team, so that they can make sure the F/E and B/E connect.\nBy working through the problem space from end to end with small piece of code, we can discover all the as-yet-unknown unknowns. In turn, that will help us ensure we properly scope the MVP. This is incredibly useful when hooking up infrastructure systems in the government space because it is common for the most technical unknowns are in how to hook the systems together/move data from point A to point B.\nFAQ # What About a COTS Project? # The steel cable is explicitly for building a new application from scratch. Sometimes, however, we are working with COTS or GOTS (Commercial or Government-Off-The-Shelf), like ServiceNow. In those cases, COTS is a box on the architecture diagram. You’re still going to need to figure out the deployment pipeline and, in theory, your COTS solution will need to talk to other services.\nWhat About Delays in Gaining Infrastructure Access? # We often face delays in gaining access with our clients. We expect the granting of access to be a primary focus of the teams’ stakeholder. The team should implement what is possible given the access available, and find creative solutions to the remaining challenges. For example, if you do not have a GitHub account, maybe it would be possible to use a private GitHub repo and switch when the official one becomes available.\n"},{"id":38,"href":"/docs/documentation/adr/","title":"Architectural Decision Records","section":"Docs","content":" Archetectural Decision Records # The 805th SHOC uses archetectural decision records(ADRs) to document engineering decisions. These include choices about composition of the tech stack, using one module or library over others, infrastructure, features, etc. \u0026ldquo;Architectural\u0026rdquo; should be interpreted broadly: any decision that could impact the project at the archectural level is a candidate or an ADR.\nADRs are useful for recording context with decisions that may become unclear over time, or as engineers rotate on and off the project. Therefore, the best time to write an ADRs is shortly after - or before! - Making the decision it documents.\nWe also encourage writing ADRs when a decision is made not to do something, as these decisions are often no less significant.\nWe typically use the markdown ADR format, or mADR. When Starting a new project or repository, review the mADR documentation.\nWriting a new ADR # to begin a new ADR, check out a branch and copy the template. You can add new ADR\u0026rsquo;s indexed by a simple sequence of integers, e.g. 0001-my-new-adr.md. In busier projects, it may be useful to use a unix timestamp to avoid index collisions when more then ond ADR is being drafted Simultaneously:\ncp docs/adr/0000-template.md docs/adr/\u0026#34;$(date %+s)\u0026#34;-my-adr-title.md Changing an old ADR # In General, once an ADR is added in an \u0026ldquo;accepted\u0026rdquo; state, it should not be changed. IF the decision is documents is later reversed, publish a new ADR explaining the decision, and change the status of the old ADR to \u0026ldquo;superseded\u0026rdquo; and outlined in the template.\n"},{"id":39,"href":"/docs/compliance/authorization-to-operate/","title":"Authorization to Operate","section":"Docs","content":" Authorization to Operate (ATO) # The Official management decision given by a senior organizational official to authorize operation of an information systems and to explicitly accept the risk to organizational operations (including mission, functions, image, or reputation), organizational assets, individuals, other organizations, and the Nation based on the implementation of an agreed-upon set of security controls.\nATO Overview # Every federal information system must go through NIST\u0026rsquo;s Risk Management Framework (RMF) before it can be used to process federal information. This process culinates in a signed Authority to Operate (ATO) being issued. Because the ATO process is a complex, multi-step process which will constrain the design and implementation of your systems, you should start thinking about how it applies to your system before you begin designing and implementing it.\nDefinitions # Assessment: The step of the ATO process (and RMF) where the system and package are reviewed by a third party. ATO package: The SSP and other documentation needed to get an ATO. Authority to Operate (ATO): The approval for the government system to be run in production, and the compliance process for getting there. Compliance: Ensuring that a system meets minimum security requirements. Information system: a discrete set of information resources organized for the collection, processing, maintenance, use, sharing,dissemination, or disposition of information ( 44 U.S.C. 3502). POAM: Plan of Action and Milestones. They are the TODOs following and assessment, which are usually low-risk security findings that need to be addressed. RMF: The NIST Risk Management Framework (RMF), which is what most ATO processes are based on. Security: The sum of processes and features safeguarding systems and data. Systems Security Plan (SSP): The primary document in an ATO package, the bulk of which contians the NIST 800-53 security controls. \u0026ldquo;The purpose\u0026hellip;is to provide and overview of the security requirements of the system and describe the controls in place or planned for meeting those requirements.\u0026rdquo; ( NIST SP 800-18) Roles # Roles in ATO process typically include:\nAssessor: Responsible for checking the compliance of systems; sit in an agency\u0026rsquo;s Security team. Validates and verifies that the documented controls (see Step 3)actually work, using assessment cases (see Step 4). Authorizing Official (AO): Responsible for overall impact categorization and risk acceptance. The AO is ultimately responsoble for determining if the risk of operating the system is acceptable, and if so, issuing an Authority to Operate (ATO) for that system. They often Designate this responsibility to one or more people. Information System Security Officer (ISSO): Supports the information security team, consults on control selection, organizes scanning process. Reports to the Information System Security Manager (ISSM). Penetration tester(s): Conducts the penetration test after terms are agrees to as a documented in the Rules of Engagement (RoE). Program team: Those who are trying to build/launch the system. System Owner: The system owner is usually the product lean or tech lead of the project team. They will be named in the ATO documents and are the main contact during the evaluation process that leads up to an ATO. FISMA # in the Federal government, the principle law governing the security of information systems is the Federal Information Security Management Act (FISMA). For more information of FISMA, check out the FISMA Ready introduction.\nOne of the goals of the Federal Information Security Management Act of 2002 (FISMA) is to “provide a comprehensive framework for ensuring the effectiveness of information security controls over information resources that support Federal operations and assets.” The National Institute of Standards and Technology (NIST) was tasked with designing and implementing this framework: the result is NIST’s Risk Management Framework (RMF). All federal information and information systems (except classified information and national security systems) are subject to NIST’s RMF. There’s an introduction to the RMF on NIST’s website. A more comprehensive guide, including how to apply the framework, references to the various relevant publications, and definitions of roles and responsibilities, is found in NIST’s Special Publication 800-37.\nRe-authorization # Your system may need to be reassessed and re-authorized if your application team is planning to make substantive changes, such as changes to:\nEncryption methodologies Administrative functionality within the application The kinds of information you store (for example, personally identifiable information (PII)) The external services used or how/what data flows to/from them Anything that will requires an update to the System Security Plan, system diagram, etc. Example changes that do not require re-authorization, as long as they don’t include the above:\nFeatures Bug fixes Interface changes Documentation updates The Authorizing Official determines whether a system needs re-authorization. If you’re planning a change that you think may require re-authorization, contact them.\nIf it needs re-authorization, follow the usual steps for getting an ATO. You should be able to reuse most of your existing ATO materials, assuming they have been kept up-to-date.\nATO renewal # Many ATOs are issued with a time limit, often this expiration is between one and three years. When an ATO nears expiration, you’ll need the ATO to be renewed. Follow the usual steps for getting an ATO. You should be able to reuse most of your existing ATO materials, assuming they have been kept up-to-date.\nTypes of ATOs # For all federal agencies, the Risk Management Framework (RMF) describes the process that must be followed to secure, authorize, and manage information systems. The RMF defines a process cycle that is used for initially securing the protection of systems through an ATO and integrating ongoing monitoring.\nAuthorization to Operate (ATO) - This is the end goal for every system. These ATOs are typically issued for 3 years, at which point they must be recertificed.\nConditional ATO (cATO) - This is usually issued to a system where too many outstanding issues were found. This allows the system to operate while the problems are corrected. These problems are tracked and become conditions that must be met at specific time intervals. cATOs are usually issued for less than a year.\nInterim Authority to Test (IATT) - An IATT provides provisional authority to operate during system development. During this time period, teams should be working towards implementing controls and documenting the system for their ATO assessment.\nSteps of the ATO Process # “The ATO process”, as it’s commonly called, is formally defined in the National Institute of Standards \u0026amp; Technology (NIST)’s Risk Management Framework (RMF): The steps in the process are as follows:\nStep 1: Categorize Information System # The information systems’ owner, working with the AO, categorizes the system based on the potential impact on the organization if the information system, or the information within it, is jeopardized. The system will end up with a category of low, moderate or high, based on criteria described here.\nIf your system will be providing novel or risky functions, or handling extremely sensitive data, do this as early as possible.\nStep 2: Select Security Controls # “Controls” are individual security requirements laid out by the National Institute of Standards and Technology (NIST). NIST’s encyclopedic Special Publication 800-53 (currently on revision 4) is the definitive guide to security and privacy controls for federal information systems.\nYour AO determines which controls need to be implemented. This is generally based on the following:\nThe impact level of the system (low, moderate or high). SP 800-53 provides a “baseline” set of controls for each level. The higher the level, the more controls or control enhancements are in scope. For systems running on cloud infrastructure, you should consult FedRAMP’s security control documentation. Which controls are already taken care of by your infrastructure. If you’re running in the cloud, many controls are taken care of at the infrastructure or platform layer. If your provider has received a FedRAMP P-ATO, it will provide a document called a customer responsibility matrix (CRM) or control implementation summary (CIS) listing the residual or hybrid controls that are the responsibility (or partial responsibility) of the applications running on the infrastructure or platform. What type of ATO you want to receive. The options will be specific to the organization doing the authorizing. Tailoring. The information system owner, working with the AO and the agency’s information security team, can then add, remove or modify controls to achieve cost-effective, risk-based security, based on the agency’s mission or business need. This step should happen as an integral part of any system design activities. The team should also develop a monitoring strategy to ensure that security controls continue to be effective once the system receives its authority to operate.\nStep 3: Implement Security Controls # As part of system development work, controls are implemented. The implementation is documented in the SSP.\nThis step is essentially “state how your system meets each of the regulations”. Using established web frameworks (Rails, Django, etc.) and hosting in a platform takes care of a lot of the lower-level controls and security best practices for you, so you only need to be concerned with your application’s custom code and configuration. This custom code and configuration is known as the “attack surface”. The final version of this document is called the System Security Plan.\nFill out the documentation. The full list of data and functions in and of the system (in government parlance “mission based information types” and “delivery mechanisms”) must be itemized in structured data. While the data types are obviously arbitrary and custom to each system we produce, the government has a formalized data set of mission functions that should be mapped to the system via NIST 800-60. For a Rails app, for example, this can simply be a link to the db/schema.rb file on GitHub.\nStep 4: Assess Security Controls # In other words, “verify that your system is secure”.\nBefore your system can go live with government information, your contol implementation must be tested. Testing is often performed by the development team and infrastructure team working together with the agency’s information security team, based on a security assessment plan. The security assessment plan will depend upon the type of ATO. FedRAMP has a Security Assessment Framework for FedRAMP ATOs.\nThere will usually be a penetration test conducted on the system. Any penetration test findings deemed serious enough to prevent an ATO will need to be fixed right away to unblock the ATO process. They will also review the SSP document and test the control narratives. This testing and review process will take 1-2 weeks and should be the top priority for the project team at the time.\nThe results of the assessment are documented in a security assessment report (SAR). Depending on the findings of the security assessment, remediation work may have to take place before the system receives its ATO. Other problems that are less critical can be remediated at a later date: these are listed in a document called a plan of action and milestones (POAM or POA\u0026amp;M).\nStep 5: Authorize Information System # The SSP, SAR and POAM together form a security authorization package (FedRAMP requires a further document: a continuous monitoring strategy). The Authorizing Official will make a risk-based decision whether to grant an ATO based on the information in this package. This decision, made in consultation with other key stakeholders such as the CISO, balances security considerations with the mission and operational needs of the agency.\nOnce all of the materials are prepared and testing is done and the system is considered “ready” by the Authorizing Official, they will sign the ATO memo. If an ATO is granted, an authorization decision document is issued and signed by the AO which lists the conditions under which the ATO will remain valid, including the ATO’s expiry date.\nStep 6: Monitor Security Controls # Once a system receives an ATO, it must be assessed at regular intervals to ensure the effectiveness of the control implementation. Any changes to the system’s security boundary or its environment should also be assessed to determine their impact.\nThere are several ways to ensure that your system remains compliant:\nPerform regular, automated security scans on your system, and act on the findings in a timely manner. Keep your System Security Plan (and any other architecture/security-related documentation) up-to-date. Security Categorization # Overview # The impact level of your project is very important, and affects the process you’ll follow to launch your project. At first you’ll be making a prediction of your project impact level. As you enter the ATO assessment process the impact level will be determined with the help of your AO.\nThe impact level is determined by the functionality of the system and the data it contains. The methodology defines three security objectives of the system: confidentiality, integrity, and availability. These security objectives are assigned one of three impact levels: low, moderate, or high. This process is described in NIST’s FIPS 199 publication.\nOnce the potential impact on these three objectives is determined, the overall impact level of the system is determined based on the “high water mark” principle. This process is described in NIST’s FIPS 200 publication.\nDetermining the impact levels is ultimately subjective; the AO makes the final determination.\nCategorize using the 3 security objectives # Go through each of the security objectives and determine the impact on the organization or individuals if the system is compromised. The framework we usually use is to ask ourselves (and the agency we are creating the system with) three worst case scenario questions:\nWhat is the worst possible outcome if all of the confidentiality of the system is lost? i.e. What if all of the data in the system is exposed to the public? What is the worst possible outcome if all of the integrity of the system is lost? i.e. What if an error makes it into the data? What if an update to the data is lost? What is the worst possible outcome if all of the availability of the system is lost? i.e. What if the system has downtime? If the potential impact is a limited adverse effect on organizational operations, organizational assets, or individuals, we select “low”. If the potential adverse impact is serious, we select “moderate”. If the potential adverse impact is severe or catastrophic, we choose “high”.\nThe answer to each question should then be interpreted in terms of impact to either the public or the government. The higher value for either impacted party is be used.\nConsiderations # The canonical or singular nature of a function being provided by the system must be taken into consideration in the categorization. The more singular and canonical the system under evaluation is, the higher the impact level.\nFor example, if we re-post data from weather.gov, it is less impactful for us to lose availability than it is for weather.gov itself. Conversely, GSA is the only source of FedBizOpps data - therefore our availability is much more important for that data and function, and we should select a higher impact level for availability.\nIf there is any authorization or authentication being done, it is likely at the moderate level for all metrics.\nJust because we need availability: high, doesn’t mean it needs confidentiality: high or integrity: high. These determinations are important for later tailoring of system controls.\nPII # Privacy risk is partially assessed based on to the degree to which a program or information system collects and makes use of personally identifiable information (PII). Per OMB Circular A-130, PII is “information that can be used to distinguish or trace an individual’s identity, either alone or when combined with other information that is linked or linkable to a specific individual.”\nStoring PII always raises the level to at least moderate for the confidentiality and integrity objectives.\nSelecting the overall impact level # Once you have decided on the impact level (low, moderate, high) for each of three objectives (confidentiality, integrity, and availability), you must then determine the overall impact level of the system. A low impact system is one in which all three of the security objectives are low. A moderate impact system is one in which at least one of the objectives is moderate, and none are high. A high impact system is one in which at least one objective is high.\nFor more information, see NIST 800-18:\nTable 1 for FIPS categorization Section 3.13 for security controls General Tips # As soon as you have a stable server that isn’t changing its security boundary (talk to the project developers about this, but it can be very early on), you should start this process. As long as there aren’t those significant changes, the tests will run periodically on any updates you make. At the very latest, start this process at least two months before launch. Do not commit to a launch date without coordinating with your AO on this first. One thing that will help the process is great documentation, which can mitigate some problems from occurring during the ATO process. Documentation, and specifically your README, should reflect a high level narrative of the architecture and data flows of the application. Questions to consider include: What does it do? How does it move information around? What does it accomplish by doing it? The DigitalGov team at GSA has collected a list of Requirements for Federal Websites and Digital Services that you should familiarize yourself with. Exactly how big of a splashy launch are you planning? Is POTUS announcing it? Have you figured out what level of traffic you need to support? This should be coordinated between the engineers on your team, your client, and the Infrastructure Team. System Security Plans (SSPs) # Remember that the reviewer knows nothing about your system, and likely doesn’t have software development background. The purpose of the SSP is to get the entire system and everything security-related around it into the brain of the reviewer. Filling out the SSP is hard, and will likely be the most time-consuming part of the ATO process. Sections 9, 10, and 13 are the hard/important ones to fill out. Focus on these first. It will be easiest to fill out your SSP while going through side-by-side with a recent SSP, ideally for a similar system. Looking at another SSP will help you understand the language/detail required. Reuse/adapt content from previous SSP(s) whenever possible. When filling out the SSP, try taking a rough first pass, and flesh it out later. Don’t Repeat Yourself. Lots of controls and sections have overlap - you will be tempted to restate the same thing multiple times. If this seems to be the case, reread the question carefully to be sure. The SSP template authors choose their words carefully. Rather than repeating the same thing across multiple controls/sections, give a brief summary with the core details of who’s responsible and how the control is fulfilled, and then cross-reference the more detailed explanation in the other place. Maintain consistency. Inconsistency can confuse the reviewers, forcing them to come back and say “well, which is it?” which slows down the process. Be especially careful to be consistent with terminology, using the exact names from the following tables throughout your ATO materials: User Roles Software Components Refer to specific User Roles and Software Components in Title Case. Only include information about your [soon-to-be] production system. Other environments (development/staging) are out of scope. System/network diagrams # One of the requirements for an SSP (and the Rules of Engagement) is to include a network diagram for your system. Some tips:\nThe diagram should be as detailed as possible. The boxes in the diagram should roughly correspond to the rows in the Software Components tables. Include all external services, such as: The Digital Analytics Program New Relic The arrows should correspond to rows in the Ports, Protocols, and Services table(s), with labels of the format \u0026lt;protocol\u0026gt;\u0026lt;port\u0026gt;(\u0026lt;T(CP) or U(DP)\u0026gt;) - \u0026lt;Purpose\u0026gt;. Example: HTTPS 443(T) - uploading files Include a visual “ATO Boundary.” A dotted line box is a nice way to do this. The system diagram includes things outside of the ATO boundary for context. Delineating the parts of the diagram being ATOd versus the parts that exist for context (such as the cloud.gov router) is helpful for reviewers. "},{"id":40,"href":"/docs/develop/direnv/","title":"direnv","section":"Docs","content":" direnv # Overview # direnv is an MIT-licensed command-line tool that addresses the hassle of setting per-project environment variables.\nInstallation # Via homebrew:\nbrew install direnv Getting started # Create an .envrc file in a directory with some environment variables your project needs:\n$ cat \u0026lt;\u0026lt;ENVRC \u0026gt; .envrc export DB_HOST=localhost export DB_PORT=5432 export DB_USER=postgres export DB_PASSWORD=mysecretpassword export DB_NAME=dev_db CLIENT_AUTH_SECRET_KEY=$(\u0026lt;client_auth_secret.key) ENVRC $ On first run, you should get a message indicating that you will have to explicitly authorize direnv to load the file:\n$ cd ./projectdir direnv: error .envrc is blocked. Run `direnv allow` to approve its content. $ direnv allow direnv: loading .envrc direnv: export +DB_HOST +DB_NAME +DB_PASSWORD +DB_PORT +DB_USER -PS2 $ Your local environment variables should be updated now. Any time the .envrc file is changed, you will need to re-approve the file, but it will load automatically otherwise.\nLanguage-specific settings # direnv and golang # From the direnv wiki:\nIf you\u0026rsquo;re using the gb build tool for Go projects, you may find this layout useful for helping many GOPATH-dependent ecosystem tools work seamlessly in your projects:\n# Usage: layout golang # # Sets up environment for a Go project using the alternative gb build tool. Also # works with the official dep package.In addition to project executables on # PATH, this includes an exclusive, project- local GOPATH which enables many # tools like gocode and oracle to \u0026#34;just work\u0026#34;. # # http://getgb.io/ # https://golang.github.io/dep/ # layout_golang() { export GOPATH=\u0026#34;$PWD/vendor:$PWD\u0026#34; PATH_add \u0026#34;$PWD/vendor/bin\u0026#34; PATH_add bin } Add this to your ~/.direnvrc and then use layout golang in your project .envrc\u0026rsquo;s. With this loaded, the support for many tools like gocode, oracle, godoc, etc. in editor plugins like vim-go and GoSublime will usually \u0026ldquo;just work\u0026rdquo;. This layout may also be completely applicable if you are using the Go 1.5 \u0026ldquo;vendor experiment\u0026rdquo; with the official go tool on your project.\nSee this pull request for some background discussion.\ndirenv and Docker Machine # From the direnv wiki:\nWhen using Docker Machine on OS X, there\u0026rsquo;s an incantation that needs to be run in every shell where you want to run Docker. Direnv can help mitigate that annoyance:\nAdd to ~/.direnvrc\nuse_docker-machine(){ local env=${1:-default} echo Docker machine: $env local machine_cmd=$(docker-machine env --shell bash $env) eval $(docker-machine env --shell bash $env) } Any project that\u0026rsquo;s using docker, add to .envrc:\nuse docker-machine One nice feature to this is that you can have different Docker machine VMs per project, if needed by saying use docker-machine special, for instance.\nNote that when using this pattern, when docker-machine warns you to eval (docker-machine env), you\u0026rsquo;ll want instead to direnv reload.\ndirenv and python # If using pipenv, add layout_pipenv to the .envrc and the virtualenv will be loaded automatically when you enter the project directory.\nStoring secrets with direnv # Sometimes, we want to put the .envrc in source control but also use direnv to store secrets that should not be source controlled. With this bit of code, we can separate local secrets into .envrc.local:\n$ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; .envrc if [[ -s .envrc.local ]] ; then source_env .envrc.local fi EOF $ echo \u0026#34;.envrc.local\u0026#34; \u0026gt;\u0026gt; .gitignore Commands in .envrc.local will be loaded by direnv, but ignored by git.\n"},{"id":41,"href":"/docs/project_management/agile-project-discovery-phase/","title":"Discovery Phase","section":"Docs","content":" Discovery Phase # Before you commit to building a product or service, the team needs to understand the problem that needs to be solved and who it needs to be solved for.\nThat means learning about:\nwho your various sets of users are - for example, a business stakeholder who is receiving the data processed by your tool, which is owned by the inventory team your users and their goals the underlying policy intent you’ve been set up to address - commonly, the client’s contract objectives constraints you may face making changes to how the product or service is run - for example, because of technology or legislation what work has been done before - for example, has there been research into this problem space before? opportunities to improve process and collaborate - by sharing data with other teams, for example While there are many activities that can take place during discovery, the following activities are foundational:\nExploratory research, where we gain a deep understanding of: business needs user goals technical needs of a proposed product Evaluative research, where we test and prioritize solutions through deep empathy for business, users, and technology. You should not start building your service in discovery, but you can start laying the technical foundations of the service. Before doing so, however, it’s important to be aware of any compliance requirements and understand potential integrations with the client’s existing technical solutions.\nWhat you learn during discovery can help you work out whether you want to move forward to a prototyping phase where you are evaluating possible solutions or hypotheses to solve the problem you identified in discovery.\nHow Long Should an Initial Discovery Take # There’s not a prescribed time period for an initial discovery phase, as the scope and complexity of the problems you are trying to solve and project constraints will impact how long you spend on discovery. Depending on the project, the initial discovery phase may last a few weeks or a few months. However, if there is a significant constraint on time due to contractual obligations or client expectations, it’s vital to discuss how that and other constraints will affect the scope of research and to set reasonable expectations upfront.\nFactors that impact the length of discovery:\nSize and makeup of your team Client’s ability and comfort level enabling discovery Knowledge of the problem space Existing research (or lack thereof) Number of known stakeholders and users who will be involved in discovery Ease of access or scheduling External and internal conditions outside of your control (ex. sudden policy changes) Examples of lengths of discovery we’ve had:\n6 month discovery in a 12 month base contract with 12 month option year 3 month discovery in a 9 month base contract with 12 month option year 5 weeks of discovery in a 12 week contract solely focused on discovery and prototyping Although you will repeat discovery continuously throughout a product lifecycle, it’s important to timebox it in the initial phase.\nSet a Goal for Your Discovery # It’s useful to start by setting a clear goal for your discovery. This will help you scope your discovery appropriately and work out when it’s finished.\nHow to Set Goals # As a team, determine goals for discovery, including when discovery is “done”. These may flex a bit over the course of discovery, but seek to set clear, achievable goals. For example, one goal or definition of done might be:\nWe understand the types of problems end-users have with paper forms. We have a picture of the range of complexity across paper forms. Note: discovery is a continuous process throughout the product development lifecycle as we will be looking to respond to changing user needs, business goals, and the product landscape. The goal of this initial discovery is to ask ourselves “what does the team need to learn in order to make research-informed decisions on how to move forward?” and “how can we deliver and test ideas with users?”\nDefine the Problem # At the start of your discovery, you might be presented with a pre-defined solution by a client stakeholder. For example, the problem is not: “We need to build an interactive map to show people where our offices are.” It’s probably something like: “People have a hard time finding the offices nearest to them when they need to book face-to-face appointments.”\nBefore you start your research, you’ll need to interrogate that solution and reframe it as a problem to be solved. This will help you better understand what your team has been set up to achieve.\nBreak down assumptions and ask lots of questions. Reframing the problem also includes agreeing what is not part of the problem.\nSo start by defining the problem you’re working on. The better you define it, the better the potential solutions you’ll end up with.\nAlso consider quantifying the value/impact of solving the problem you’ve been set up to address. During discovery, that means understanding how much the problem is currently costing or impacting the users, workflows, and/or systems. Understanding this and aligning across teams, stakeholders, and clients, will help focus everyone on how to weigh the trade-offs you uncover during our research and development.\nHere are some helpful resources to guide setting goals:\nThe Actual Problems to be Solved by Kate Tarling An in depth guide to measuring the benefits of your service from gov.uk What to Find Out in Discovery # Once you’ve set a goal for your discovery and understand the problem you’re looking into, you’re ready to define research questions and begin research. Read more about the difference between research and interview questions.\nResearch Questions # Research questions will articulate the key questions the team seeks to answer during the discovery phase, in service of the client/project goals.\nHere are examples of research questions:\nResearch Questions are Not Interview Questions by Erika Hall As you articulate your research questions, be sure to focus on areas such as the following:\nlearning about your users and their context understanding the constraints that affect your problem (e.g. technical, policy, people, etc.) unpacking the wider context, including existing systems, programs, competitors, etc. that impact your users and goals identifying opportunities and barriers While discovery is most effective when there is cross-practice participation, there may be certain practices or skill sets best suited to leading certain areas of inquiry. For example:\nUnderstanding user needs is often led by design Understanding the market, product/business landscape, and product analytics is often led by product. This includes understanding policy requirements and competitors, and developing product analytics Understanding the technical landscape is often led by engineering. This may include learning how to deploy a guide-wire app in the client environment, documenting other neighboring apps and integration points, or gathering security requirements (ex. Authority to Operate). Understanding Users and Their Context # Start by learning about your users and their context. This means understanding what the user’s trying to achieve and how they go about doing it.\nWhen you dig into this, you’ll often find the thing you’re working on is part of a bigger process or user journey.\nUnderstanding context includes developing a picture of what that wider journey looks like - for example, by creating a service design blueprint of a user’s journey and the systems and programs that exist in a wider context.\nAs you build out your map, you’ll probably notice that the problem spans across multiple departments, people, and systems. Spend some time during discovery learning from those other teams and organizations.\nAccessibility and Inclusion Practices # You’ll also need to learn enough about your users’ accessibility requirements to let you work out whether the problem space you’re looking at presents any particular challenges from an inclusion point of view. Government clients require section 508 compliance for all government software. We strive to maintain a minimum level of accessibility for all products delivered from SHOC.\nBear in mind that, in the US, 1 in 4 people have some kind of disability. And that accessibility covers a range of other needs for people who do not have a disability. When you design for disabilities, you make things better for everyone in the process.\nYou’ll also need to think about things like your users’ digital skills, devices they have access to, and internet access (i.e. in rural US, “nearly one fourth of the population lack access to fixed broadband service at threshold speeds”).\nUnderstanding Constraints # You’ll need to understand any constraints you’re likely to come up against if you were to move on to the prototyping phase. This includes constraints due to:\nusers legislation/policy contractual obligations (such as imposed milestones or deliverables) legacy technology or technical requirements existing processes and systems client staffing or skill set constraints (such as for maintaining a product or service in future) There are multiple kinds of constraints: hard and soft.\nYou will not be able to do much about hard constraints. For example, congressional legislation is not something that can easily be changed. But as you move on to other phases, you’d need to find a way of delivering a service that still works for your users.\nSoft constraints may be changeable. For instance, if existing processes (for example, an existing technical stack requirement) are preventing you from delivering the best version of your service, you’ll need to work to change those processes - not just work around them and think “it’ll work out.”\nUnderstanding constraints is helpful for two main reasons. First, it helps you work out how to continue to future phases. If there’s a hard constraint which means you are not able to improve on the solution that’s currently available, this is crucial to communicate with your team and client to determine how to move forward.\nThe second reason is that it can help you prioritize your risky assumptions as you continue on. For example, if a service will only be viable if you can change an existing process or structure, you’d want to focus on ways of doing that.\nYou could also look at related or similar services, to understand the constraints they face and how they dealt with them.\nHow to Identify Constraints # Unlike risks, constraints have a clear impact on your work. A risk might happen and you need to assess how likely that is to occur, the potential impact if it does, and what you might do in case it happens (i.e. mitigation plans). Constraints can be identified and will shape your solution ideas and help the team understand what might be more/less feasible, valuable or impactful.\nAn exercise that will help your team explore what are hard and soft constraints is the Sphere of Influence exercise in the Project Toolkit.\nIdentify Improvements You Might Be Able to Make # One of the benefits of understanding the user’s wider journey and who’s involved in delivering it is that you can spot things that could be improved. You could take these improvements on to later development phases and raise them to major stakeholders.\nFor example, your discovery research might reveal that another part of the company or government is already collecting the personal information you need from your users. If you decide to go ahead and build a service, reusing that data would prevent users from having to provide the same information multiple times.\nIn that case, you could spend part of your framing process evaluating the technical and legal challenges of reusing that data in your product or service rather than the challenges of collecting it.\nYour research might also lead you to consider alternatives to building a service. For example, you might be able to solve the problem more effectively (or less expensively) by publishing website content, giving improved information to face-to-face advisors or making data or an Application Programming Interface (API) available to third parties. This is a good time to align with your team and clients to determine how you move forward.\nHow You’ll Measure Success # Early on, you should establish how your client defines success and work with your client to identify measurable success metrics that align with their goals. This will enable you to measure success when you are ready to start validating potential solutions.\nDuring the discovery phase, try to establish baselines for their current products, services, or processes for those success metrics so that you have a starting point from which to measure success of your future prototyping and beta phases.\nIn addition, you’ll need to consider:\nwhat data you’ll collect to measure service performance what performance metrics you’ll use to understand if the service is working for users what frameworks will work for you and the client Sharing What You Learn # “Help people follow us” and “Build alliances” are two core SHOC values. The way teams demonstrate this is to work openly and transparently both within our teams and with our clients and partners. You can follow this by proactively sharing research, inviting participation, asking clarifying questions, and speaking candidly with kindness.\nThis is especially crucial during research phases. Not every team member can be in every single research activity; you will likely have multiple workstreams of discovery going at the same time. Team members should share what they’ve learned across the team to encourage a common understanding of the project. Share outs might include documentation, demos, synthesis sessions, or discussions. A benefit to sharing your work early and often is that is also helps your team develop trust and confidence with our users, stakeholders, clients, and Rhinos. This all leads to “Make good coffee.”\nSharing Work with the Client # At a minimum, you must share out work with your team and stakeholders.\nTransparent Communication Examples # Post to internal client sites where clients can follow along. (See example from Managed Care Review where business owners across CMCS can view the project and progress) Post findings publicly in shared Slack channels, etc. to work transparently and inform clients and other vendors Utilize a centralized repository to hold all recorded interviews, workshops, document reviews, etc. in a tool like Dovetail or Confluence Provide your team and clients with short readouts during discovery to share what we’re hearing Sharing Work Externally # Unless confidentiality issues mean you cannot, talk publicly about what you’re learning. You could do this by publishing blog posts or running open show and tells. You can usually ask your project’s delivery manager or client success manager about what information can be publicly shared.\nThis helps people across and outside your organization know what you’re doing and makes it easier to collaborate with the other organizations working in your problem space.\nHow You Know Discovery is Finished # You will want to develop a definitions of done for your initial period of discovery on a new project. For example, this will likely include that you:\nunderstand the client’s business objectives and who the users are have defined and validated the problem(s) you are trying to solve for your users understand the wider context and know the other services, teams, competitors, and organizations working on similar problems understand the technical landscape and the technical and policy constraints have a list of solution ideas you’d like to test at alpha and an idea of which one you’d like to test first identify how you’ll define and measure success to validate ideas during the alpha phase Here is an exercise you can use to determine a definition of done for Discovery. Your initial round of discovery might conclude when you achieve your stated discovery goals and decide to validate and refine your solution ideas through alpha testing, but the discovery process does not end there for a project team. You should make discovery a continuous, repeatable process.\nAccess and Research into Systems # As contractors, access to client systems and documentation may be slow, and it’s best to start this process early.\nBegin with organization wide systems, which may be hard constraints. For example, organizations may require use of a single sign-on solution (like login.gov). As discovery progresses, and problems become more defined, look deeper into the systems affecting those problems or ones that are adjacent. For example, you may require access to an existing data platform for use or migration. Starting this work early will allow you to get access in time to inform constraints and timelines for most effective solutions.\nPreparing for Prototyping # Your initial discovery is finished when you’ve decided whether or not you are ready to move on to prototyping. There a few factors that play into this decision, including whether:\nthere’s a viable service you could build that would make it easier for users to do the thing they need to do it’s cost effective to pursue the problem - this means weighing up how much it’d cost against how much of an improvement you think you could make It’s not a failure to realize at the end of the discovery phase that your research shows what you were tasked with is not the best thing to do. This is a great time to align with the internal and client/partner teams to discuss what you have found and where to go from here. Related guides # You may find these guides useful:\nHow the prototyping phase works How the beta phase works How the live phase works Retiring your service Source Credit: This content is an edited reprint from the GOV.UK Service Manual under the terms of the Open Government Licence v3.0.\n"},{"id":42,"href":"/docs/compliance/information-security-and-federal-compliance/","title":"Information Security and Federal Compliance","section":"Docs","content":" Information Security and Federal Compliance # The Federal Information Security Management Act (FISMA) was passed in 2002 and made a requirement for federal agencies to impliment cybersecurity programs to protect systems and information. FISMA requires federal agencies to create and embed IT security plans, including policies for the IT risk assessment. Fisma applies to federal informationsystems and networks but also information assets that are processed or managed by government contractors, and subcontractors too. FISMA promotes taking a risk-based approach to protecting information security across federal networks. This way cyber security protection scales along with the tisk of harm resulting from a potential breach.\nA risk-based approach provides an insight into the best investment in time and resources. In practice, FISMA sets out a series or requirements which includes meeting specifics NIST standards around cyber security policy and procedure. FISMA was amended and modernized 2014 with the Federal Information Security Modernization Act. Often referred to as FISMA 2014, the amendments reformed the way compliance is reported amongst other changes. This guide explores the background of FISMA, what it means for federal cybersecurity, and ways to maintain and achieve compliance.\nAuthorization to Operate # ATO - Authorization to Operate ATO Templates - ATO Template Documentation Risk Management Framework # RMF - Risk Management Framework "},{"id":43,"href":"/docs/documentation/introduction-to-documentation/","title":"Introduction to Documentation","section":"Docs","content":" Introduction to Documentation # Documentation is a key part of any sowftware project, but it\u0026rsquo;s often something that many engineers struggle with even when we recognize its importance. Writing well is just as difficult whether it is code or docs, and both require deliberate effort in order to hone your skills.\nDocumentation has a positive feedbackloop \u0026ndash; if you have documentation that is well-written, provides real value, and is easy to find, you\u0026rsquo;re much more likely to have people work to keep it acurate and upto date. In contrast, if you have documentation that is inaccurate, hard to read, and/or hard to find, then people will tend to discount the value of documentation entirely \u0026ndash; so they are much less likely to treat it as a key component of your project.\nDocumentation Tips # Make your documentation easy to find # Docs that no one knows about or can\u0026rsquo;t find when they need them are essentially docs taht done exist. If we want to encourage people to write good docs, we need to make sure we can model good behavior. Some specifics tips for accomplishing this goal:\nEstablish strong norms(if not hard and fast rules) about where documentation belongs, preferably in no more then 2 places (eg, a Git repo for technical docs and a Google Drive for design docs). If people have to look in too many places to find what they are looking for, they will eventually just giveup (and it makes it more likely you\u0026rsquo;ll have many different inconsistent documents for the same thing). Make it easy to find documents in those repos without needing to play a guessing game with keywords (especially since a search for something like \u0026ldquo;terraform\u0026rdquo; or \u0026ldquo;docker\u0026rdquo; is probably going to give you to many results to be useful). Provide good tables of contents (they should not be long) and put related documents close to each other topologically. Give documents descriptive, non-generic titles. Giving a document a title like \u0026ldquo;Docker\u0026rdquo; or \u0026ldquo;Alerts\u0026rdquo; becomes increasingly troublesome the larger your project gets \u0026ndash; if your\u0026rsquo;re using Docker for three different applications and local development, what does the \u0026ldquo;docker\u0026rdquo; document refer to? Keep in mind the specific restrictions of your repositories. If you are using a wiki with a table of contents in the left margin that only shows ~20 characters, keep your titles as close to that limit as possible. If you are a Git repo, Make sure the filenames for your markdown douments are also descriptive. Make Documents serve a single purpose # A doument that tries to do too manythings ends up doing none of them well. If you are writing a step-by-step how-to on a specific process, avoid going into a deep dive on the involved systems. in addition, keeping your documentation focused will mean individual documents will be kept shorter, which is important if you want people to be able to read easily and find the information they are looking for.\nConsider breaking your documentation into a set of distict categories, and make sure the type of document has a very clear in the title (so you could title the document \u0026ldquo;Runbook: MongoDB Memory Alerts\u0026rdquo;, for instance). Use an [ADR] to define what these types are \u0026ndash; such as:\nHow-tos are step-by-step guides to common procedures intended for execution under normal operation. A reader should be able to execute these steps safely when following the document. Runbooks are documents intended to be used in concert with alerts that guide troubleshooting and mitigation responses and should include information on issues that users may expect with upstream and downstream services. A reader should be able to preform a quick diagnosis of an alert\u0026rsquo;s underlying causes, or at least rule out any common issues, with the information provided in the document. Guides provide a deep dive into how a specific system or service works, woth references to ADRs that illustrate how technical decisions were made on the project. Readers should come away with a strong understanding of how the system works, as well as how to gather more information about the system and its state on their own. These arent ment to be precriptive; define your categories in a way that works best for your project. However, whatever you decide, the categories should be clearly defined and focused; notice the \u0026ldquo;what the reader should get from this document\u0026rdquo; part of the description \u0026ndash; if you cannot adequately summarize what a reader is supposed to get from the document in a single sentance, it is probably too widely scoped.\nKeep your audience in mind always # The most important thing when writing documentation is to remember that you are writing this for someone. It may be new engineeers, it may be people on-call, it may be stakeholders in the c-suite, but whoever they are, you intendfor someone to be reading this at some point in the future.\nIf you aren\u0026rsquo;t keeping this audience in mind throughout the process of writing your docs,then you are going to miss the mark. Writing a document intended for new engineers and assuming a ton of knowledge will severly limit its utility. Similarly, writing something intended for project leads and spending pages defining common terms they\u0026rsquo;re already familiar with will probably cause them to lose patience and start to skim things over.\nAvoid going overboard with graphics or videos # Often we\u0026rsquo;ll be tempted to add lots of screen shots, diagrams, or other graphics to documentation to try and make things more clear. a well-placed graphic can help \u0026ndash; but to many will make the upkeep of your documentation too difficult, it is much more difficult to edit a graphic (and videos even more so) that to edit text, so use them only when they\u0026rsquo;re worth the added cost in maintainability.\nin addition, adding lots of graphics will make it harder to read the text. If people cannot follow from one paragraph to the next (seeing them both on the screen at the same time), this is especially difficult. If you need to use large praphics, use thumbnails and link to the larger version to prevent them from being to distracting. Graphics can also make your docs less accessible, so captions and alt text are important.\nKeep individual documents short \u0026ndash; but not too short # Ther\u0026rsquo;s nothing worse thatn trying to find the one paragraph you need from a 20 page document. In order to avoid this, we should keep individual documents short and focused. Generally, anything more then 4 \u0026ldquo;pages\u0026rdquo; (ie, you would need to scroll four full screens to see the whole thing) is going to feel \u0026ldquo;long\u0026rdquo; to most readers \u0026ndash; if you go over this length, be aware that a lot of people will resist reading this documentation in a single sitting. This might be acceptable for a thorough technical document, but it\u0026rsquo;s not great for something you\u0026rsquo;re expecting people to refer to regularly.\nIf your document is something people are going to have to refer to in a crisis, you should consider keepingit even shorter \u0026ndash; no more than two pages. Following the other tips on this page will help you keep your documents short and focused.\nOne Warning \u0026ndash; if you link to other documents in yours to keep your document shorter, try to avoid sending people down a wikipedia-esque link hole. If you can summarize the information you want people to get from the other document ina sentance or two, you should try to do that (in addition to providing the link for further exploration).\nGet Reviews for your documentation # You wouldn\u0026rsquo;t want to put code in production without a though code review and testing \u0026ndash; you shouldn\u0026rsquo;t expect people to use your documentation before it gets a though review either. SOme tips for reviewing docs:\nJunior Members of your team are great people to have review your docs, since they will likely be using them themost. They aren\u0026rsquo;t usually the best people to write docs though, since they have the least context for them. For step-by-step guides, try running the commands. Do they work? Is it safe to runthem? They Should not use real people\u0026rsquo;s usernames or other problems if someone just cut-and-pastes the command! Try reading paragraphs out loud. Does the language feel awkward or ambiguous? If you\u0026rsquo;re the writer, have the reviewer tell you what they took away from the document \u0026ndash; is it what you wanted them to know? Did they come away with a good mental model of the system you described? Miscellaneous # IF you use graphics, write the caption first then make sure the graphic represents what you want to convey. Lists and Tables can be a good way to present information people need in a hurry \u0026ndash; they are easier to digest than paragraphs of text (but may not be best for nuanced information). Use an active voice rather then a passive voice whenever possible (\u0026ldquo;do this thing\u0026rdquo; not \u0026ldquo;this thing must be done\u0026rdquo;). Put conditionals before imperatives to prevent people from doing something without realizing when they shouldn\u0026rsquo;t (\u0026ldquo;if not Y, do X\u0026rdquo;,not \u0026ldquo;do x if not Y) "},{"id":44,"href":"/docs/project_management/phases-of-an-agile-project/","title":"Phases of an Agile Project","section":"Docs","content":" Phases of an agile project # Two horizontal sections. The top section, green, labeled \u0026ldquo;discovery track\u0026rdquo;. The bottom section, blue, labeled \u0026ldquo;delivery track\u0026rdquo;. Each with a row of counterclockwise circles pointing to each other.\nThere are two tracks of the agile project: discovery and delivery. Workflows are handed between these two phases as insights are gained and software is delivered. Within each track are phases, which occur in repeated cycles.\nDiscovery Track # The discovery track is where you connect with your users, understand their problems, and validate any assumptions or solutions you arrive at. It happens continuously alongside development, but also sequentially with the insights and ideas you generate informing the delivery track.\nDiscovery Phase # This phase is typically at the beginning of a project engagement and also repeated throughout the product lifecycle. This phase is used to understand the people and systems who may be impacted by and involved in what you build, their needs, what problem that needs to be solved, and what may affect your work - including policy, legislation, and technology. You will begin to test a wide variety of solution ideas in this phase so you can narrow those down before moving into alpha.\nPrototyping Phase # In the Prototyping phase, you will develop robust prototypes and continue to test those out with users, clients, and your team. This is where you will implement what you have imagined, challenge your ideas, and determine whether to move forward with them.\nDelivery Track # Delivery is where you implement lessons learned in discovery. As the product is iterated on, its new state goes back to discovery to be assessed.\nBeta Phase # Beta is where you take your prioritized solution from Prototyping and start building it to be production-ready. This is the time to test the working application with a subset of users in a production-like environment, and understand how it will function when live.\nLive Phase # The Live phase is where you will deliver the service to your users and focus on maintaining and iterating on it.\nHanding off your service # What to do when your time working on the service is over.\n"},{"id":45,"href":"/docs/compliance/risk-management-framework/","title":"Risk Management Framework","section":"Docs","content":" Risk Management Framework (RMF) # For all federal agencies, the Risk Management Framework (RMF) describes the process that must be followed to se cure, authorize, and manage information systems. The RMF defines a process cycle that is used for initially securing the protection of systems through an Authorization to Operate (ATO) and integrating ongoing monitoring.\nAdopting the Risk Management Framework # The risk management framework is a framework, not a policy. Frameworks have to be implemented. The way we implement needs to overcome issues with point-in-time ATO documentation while maintaining its strengths.\nGoals # The solution an agency adopts should:\nAddress the shortcomings of assessment and accreditation. Get to ATO efficiently. Allow data and tools to flow between organization policy-setting and information systems. Continuous monitoring # OMB has directed agencies to adopt Information Security Continuous Monitoring (ISCM). Continuous monitoring requires ongoing awareness of vulnerabilities and threats. These cannot be assessed at a single point in time in order to produce an ATO and then trusted until the next ATO review or reauthorization. NIST’s Special Publication 800-137 describes implementation of ISCM.\nAn organization-wide approach # Among the requirements is whole-organization involvement:\nMaintaining an up-to-date view of information security risks across an organization is a complex, multifaceted undertaking. It requires the involvement of the entire organization, from senior leaders providing governance and strategic vision to individuals developing, implementing, and operating individual information systems in support of the organization’s core missions and business functions.\nNIST uses three tiers to describe how the organization should integrate monitoring into their systems. In Tier 1, the agency should set its risk tolerance, governance, policies, and strategies. In Tier 2, the agency should set business processes that can support these organizational goals. In Tier 3, the agency needs information systems that provide the data for these processes. Data moves from the systems into the processes into the organization. Tools and capabilities flow from the organization into the processes into the systems.\nStructured data # A modern implementation of the risk management framework needs to support this movement of information. To do this, agencies need to integrate the development of secure systems with how that security is documented. To keep data moving from systems to the organization, the agency needs to produce consistently up-to-date documentation that is reliable. To make tools available to systems at the right time, the agency needs to restrict pace of development as little as possible.\nRisk Management Framework Stages # Government information systems must follow a risk management framework described by the National Institute of Standards and Technology (NIST). The framework is a cycle of activities that an agency needs to perform on all information security systems:\nCategorize: Define the level of risk the system presents — low, moderate, or high — based on a worst case scenario Select: Choose baseline security controls and any guidance or supplement controls based on the categories Implement Put the controls in place using appropriate engineering practices. See SP 800-70 Assess: Determine how effective the controls are. See SP 800-53A Authorize: Review the risks to the system and, if they are acceptable, authorize the system Monitor: Track changes that affect security controls and reassess how effective they are Continuous monitoring leads the agency back into the cycle, reviewing whether new information requires new categories, controls, or assessment criteria. The risk management framework describes specific categories or levels of risk that a system can pose to the government, and it recommends specific baseline controls based on a system’s risk level. Agencies determine much of the process for themselves, including how to analyze risk, how to demonstrate selected controls, and whether and how to exceed the baseline recommendations. One of the key components of the framework is that the final step, monitoring, is continuous.\nOMB and NIST have directed agencies to change how they approach risk management frameworks from point-in-time accreditation and authorization reviews to continuous monitoring of information system security. Here we provide a high-level overview of why this has changed and how to make continuous review work for your systems, your development teams, and your information security teams.\nCategorize # Categorization is based on an impact analysis and is performed to determine the types of information included within the authorization boundary, security requirements for the information type, and potential impact resulting from a security compromise. Agencies are required to categorize their information systems as low-impact, moderate-impact, or high-impact for the security objectives of confidentiality, integrity, and availability and to select appropriate security controls.\nObjectives # Information systems have three security objectives defined by the Federal Information Security Management Act (FISMA).\nConfidentiality: Prevent unauthorized disclosure of information. Integrity: Prevent unauthorized modification or destruction of information. Availability: Prevent disruption of access to information or use of the system. Categorizing # To categorize the information system, you need to establish two things. First, what are the data types in the system? Second, for each of those data types, would the failure of each of the three security objectives pose a low, moderate, or high risk to the agency? Decide how damaging the loss of confidentiality, integrity, or availability would be to both the agency’s ability to work and its money, property, or people.\nRisks and risk categories # | Area of risk | Low risk | Moderate risk | High risk |\n| Ability to work | Minor but noticeable | Significant but does not affect agency’s primary functions | Harms ability to perform primary functions |\n| Money, property, or people | Minor | Significant | Catastrophic |\nExpressing the results # For each information type, state the risk for confidentiality, integrity, and availability. Structure it in this way:\nSC information type = {(confidentiality, impact), (integrity, impact), (availability, impact)}\nUse low, medium, or high for the impact. Then take the maximum impact category against each objective and use that for the information system as a whole. Structure it the same way:\nSC information system = {(confidentiality, impact), (integrity, impact), (availability, impact)}\nAn individual data type’s impact can be n/a if there is no risk at all, but for the whole system, the minimum risk level on any objective is Low.\nCategorization under continuous monitoring # You should begin categorizing information systems as close to the beginning of the system development life cycle as possible. Each time you identify an information type the system will handle, give it an initial categorization. As you assemble a larger set of them, review the initial categories and update if needed. You might find that relationships among categories raise the risk of one or more of them. You might find that the use of the system is going to change, which could change the risks. The full set of information types will produce system categories that are unsurprising. You are more likely to be prepared to select, implement, assess, and document the correct controls.\nBuilding knowledge of risk as you build the system is best done by having the developers participate. As they identify system risks, they can think about good controls rather than reacting after the fact to categorization received from information security. And the flexibility this builds into the development life cycle will make it much easier to react to issues raised by continuous monitoring.\nMore detailed reading from NIST # FIPS 199 describes the objectives, categories, and how to structure your categories. SP 800-60 recommends an specific approach to categorization. Select # Controls are the management, operational, and technical safeguards or countermeasures employed within an organizational information system that protect the confidentiality, integrity, and availability of the system and its information. The specific controls required to protect the system are based on the categorization of the system.\nSecurity and privacy control baselines serve as a starting point for the protection of information, information systems, and individuals’ privacy. NIST SP 800-53B defines these security and privacy control baselines. The three defined control baselines contain sets of security controls and control enhancements that offer protection for information and information systems that have been categorized as low-impact, moderate-impact, or high-impact.\nThese are the security and privacy control families for information systems in NIST SP 800-53 rev. 5. Specific controls and control enhancements are found within each control family.\nID\tControl Family\nAC\tAccess Control\nAT\tAwareness and Training\nAU\tAudit and Accountability\nCA\tSecurity Assessment and Authorization\nCM\tConfiguration Management\nCP\tContingency Planning\nIA\tIdentification and Authentication\nIR\tIncident Response\nMA\tMaintenance\nMP\tMedia Protection\nPS\tPersonnel Security\nPT\tPII Processing and Transparency\nPE\tPhysical and Environmental Protection\nPL\tPlanning\nPM\tProgram Management\nRA\tRisk Assessment\nSA\tSystem and Services Acquisition\nSC\tSystem and Communications Protection\nSI\tSystem and Information Integrity\nSR\tSupply Chain Risk Management\nMore detailed reading from NIST # FIPS 200 lists the minimum security requirements for controls in seventeen areas of information security. SP 800-53 provides a full list of controls and augmentations to those controls that an agency can adopt. SP 800-53B provides a full list of controls and augmentations to those controls that an agency can adopt. Implement # Controls specified in the System Security Plan (SSP) are implemented by taking into account NIST SP 800-53 Security and Privacy Controls for Federal Information Systems and Organizations and the minimum organization requirements (i.e., organizationally defined parameters).\nA team must implement the selected security controls and document all the processes and procedures they need to maintain their operation. This includes implementing the security controls and documenting the security control implementation details, as appropriate, in the security plan.\nThere are three types of control implementation:\nSystem Specific Controls - System-specific controls are security controls that provide a security capability for a particular information system only and are the primary responsibility of information system owners and their AO.\nCommon Controls - Common controls are security controls that can support multiple information systems efficiently and effectively as a common capability. When these controls are used to support a specific information system, they are referenced by that specific system as inherited controls.\nHybrid Controls - Hybrid controls are security controls where one part of the control is deemed to be common and another part of the control is deemed to be system-specific.\nAssess # The purpose of assessing security controls is to ensure they were implemented correctly, operate as intended, and successfully meet the security requirements for the information system. Assessments are required prior to system authorization and annually to ensure that the security measures are working effectively.\nA full scope assessment of all security controls must be performed prior to the initial ATO, and the ATO must be renewed every three years. Each year, 1/3 of the controls are tested so that by the end of the third year, all controls have been tested for the ATO renewal. A full scope assessment of the controls can be required if significant changes to the information system are made at any time throughout the lifecycle.\nThere are currently two approaches for completing assessments:\nA Security Control Assessment (SCA) is a systematic, manual procedure for evaluating, describing, testing, and examining information system security controls.\nAdaptive Capabilities Testing (ACT) is an agency-specific, next-generation assessment based on NISTIR 8011 that relies heavily on automation and focusing on capabilities rather than individual controls.\nNIST SP 800-53A, Guide for Assessing the Security Controls in Federal Information Systems and Organizations, has more detailed information about assessing security controls.\nNISTIR 8011, Automation Support for Security Control Assessments, has more detailed information about automated assessments and capabilities.\nAuthorize # While agencies have a lot of flexibility in how they implement the risk management framework, all of them need to include formal authorization of the system before it is operational. This is as true for continuous monitoring as it was for point-in-time documentation.\nWhat is Authority to Operate? # Authority to Operate (ATO) is the usual name for the result of this step. It is the last step in the risk management framework before a system is operational. (Monitoring happens during operation.) ATOs usually take the form of a document describing the risks to the government of that system, the controls that are in place to safeguard it, and the acceptance of the risks it presents.\nIn authorizing an information system to operate, NIST requires a senior official to explicitly accept the risks of the system and its controls. Although the term “Authority to Operate” is not described in law, the ATO is the term applied to how the official accepts those risks.\nDefining the ATO process # The Chief Information Security Officer (CISO) at each agency and the head of that agency define the processes by which the agency will review information system security. How this is structured depends on a number of things, such as the agency’s willingness to take on risk and what kind of demonstration they expect for controls. To receive an ATO, the developers of a system must show how the system meets the controls that apply to it. (How they demonstrate that is also part of this defined process.)\nTraditionally, the way to demonstrate controls is in a single document. It describes each risk and the controls in place for that risk. These documents can be dozens or hundreds of pages, and they usually describe how each control is implemented. Auditors can review these documents quickly and easily against the requirements and recommended controls from NIST, but they can be difficult to assemble from the broader systems developers have created.\nMore detailed reading from NIST # SP 800-37\nMonitor # Risk management is a continuous process. Information systems are in a constant state of change with upgrades to hardware, software, or firmware and modifications to the surrounding environments where the systems reside and operate. A structured approach to managing, controlling, and documenting changes to an information system or its environment of operation is an essential element of an effective monitoring program. Strict configuration management and control processes are established by the agency to support such monitoring activities.\nSecurity Impact Analysis (SIA) determines the extent to which proposed or actual changes to the information system or its environment of operation can affect or have affected the security state of the system. Changes to the information system or its environment of operation may affect the security controls currently in place, produce new vulnerabilities in the system, or generate requirements for new security controls that were not needed previously. If the results of the SIA indicate that the proposed or actual changes can affect or have affected the security state of the system, corrective actions are initiated and appropriate documents are revised and updated.\nMore detailed reading from NIST # SP 800-37 provides guidance for applying the risk management framework, start to end. SP 800-53A describes customizable procedures to assess how the framework, as applied, works with the agency’s risk tolerance. How you assess the framework is a key input to your ongoing monitoring processes. Authorization package # In order to satisfy an agency’s requirements for a completed ATO, a team must complete a set of documents called the “authorization package” that fully describe the security controls that are in place to protect the information system. NIST SP 800-37 defines the authorization package as:\nThe essential information that an authorizing official uses to determine whether to authorize the operation of an information system or the provision of a designated set of common controls. At a minimum, the authorization package includes an executive summary, system security plan, privacy plan, security control assessment, privacy control assessment, and any relevant plans of action and milestones.\nThe exact process and document titles vary from agency to agency, but in general the most common required document names are:\nSystem Security Plan (SSP) - A formal document that provides an overview of the security security controls, whether in place or planned, and implementation details for meeting those requirements. This document summarizes the overall approach taken to address each of the control families. Sometimes the SSP document includes the NIST SP 800-53 security and privacy controls; other agencies prefer to break the details of those out into a supplemental document.\nPrivacy Impact Assessment (PIA) - An analysis of how information is handled that ensures handling conforms to applicable legal, regulatory, and policy requirements regarding privacy. Determines the risks and effects of collecting, maintaining, and disseminating information in an identifiable form in an electronic information system. Examines and evaluates protections and alternative processes for handling information to mitigate potential privacy risks. Defined and required by Office of Management and Budget OMB M-03-22.\nPrivacy Threshold Assessment (PTA) - A questionnaire used to determine if a system contains personally identifiable information (PII), whether a PIA is required, whether a System of Records Notice (SORN) is required, and if any other privacy requirements apply to the information system. A PTA should be completed when proposing a new information technology system through the budget process that will collect, store, or process identifiable information, when starting to develop or significantly modify such a system, or when a new electronic collection of identifiable information is being proposed. A PTA will determine if a PIA is required.\nRisk Assessment (RA) - A document that identifies risks to organizational operations (including mission, functions, image, reputation), organizational assets, individuals, other organizations, and the nation resulting from the operation of an information system. Part of risk management, an RA incorporates threat and vulnerability analyses and considers mitigations provided by security controls or privacy controls planned or in place. Synonymous with risk analysis.\nIncident Response Plan (IRP) - The documentation of a predetermined set of instructions or procedures to detect, respond to, and limit consequences of malicious cyber attacks against an organization’s information system(s).\nDisaster Recovery Plan (DRP) - A written plan for recovering one or more information systems at an alternate facility in response to a major hardware or software failure or destruction of facilities.\nATO Boundary Diagram - A visual layout of the information system that clearly describes the authorization boundary. This diagram shows which technology resources are included within the ATO boundary and all external connections.\nInterconnection Systems Agreements / Memoranda of Understanding / Memoranda of Agreement (ISA/MOU/MOA) - Agreements between the federal agency operating an information system with an ATO and outside organizations. These agreements include details of sensitive information being shared and how it is being secured. These are generally included in ATO processes in order to clearly document how Personal Identifying Information (PII) is being shared between the federal agency and other agencies or third parties.\nPlan of Action and Milestones (POA\u0026amp;M or POAM) - A document that identifies tasks needing to be accomplished. It details resources required to accomplish the elements of the plan, any milestones in meeting the tasks, and scheduled completion dates for the milestones.\nSecurity Assessment Report (SAR) - Assesses the findings of the assessor and the recommendations for correcting any identified vulnerabilities in the security controls.\nRisk Assessment Report (RAR) - Assesses and documents the use of resources and controls to eliminate and/or manage vulnerabilities that are exploitable by internal and external threats.\nProblems of the Assessment and Authorization # Many agencies have created strong processes for creating an ATO document, which describes a system and its controls at a single point in time. This is useful as a compliance check, but point-in-time documentation is insufficient to modern information security needs.\nOut-of-date controls # An ATO can be a very large document that takes a long time to write, often longer than it takes new threats to evolve or new capabilities that might require new controls. The point-in-time ATO cannot describe new controls and new risks.\nLagging security reviews # ATOs define a point at which they must be reviewed or renewed, even if controls have not changed. This requires review of the whole management framework. But traditional implementation of information security did not require review more frequently. Developers might work on security improvements, but they are not included in the documents that inform compliance.\nLagging product improvements # One way to prevent out-of-date controls is to slow down the adoption of enhancements. But slowing adoption can create other problems. Issues that could have been discovered and addressed earlier are instead discovered late in development. Because so many other decisions or assumptions have already been made, they are more expensive to address and the organization has fewer choices to solve them.\nSlowing work can also leave systems lagging behind the skills of good developers. This has knock-on effects, reducing the pool of developers who want to join government, and reducing opportunities for government developers to build on their own skills.\nReduced contributions to compliance from developers # The people who develop systems think about the security of a system in terms of how the system works and what modules serve as security controls. Compliance reviews work from the opposite direction: starting from the controls and identifying which modules implement them. Information security teams can ask which module implements each control repeatedly until they satisfy a list of controls.\nIf information security teams could ask instead what security controls a particular module implements, a development team could provide the full range at once. This allows developers to think more proactively about how each module they develop functions as a security feature.\n"}]